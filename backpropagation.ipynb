{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "VRLNrFDU3dKp",
   "metadata": {
    "id": "VRLNrFDU3dKp"
   },
   "source": [
    "## Part 1\n",
    "\n",
    "### Building Model1\n",
    "Uses Pytorch to implement a three-layer Neural Network (input layer - hidden layer - output layer) and update the weights with backpropagation.\n",
    "- 1. Implement forward and calculate the output\n",
    "- 2. Calculate errors and loss  \n",
    "- 3. Update the weights with backpropagation \n",
    "- 4. Predict function \n",
    "- 5. Activation function (Sigmoid function) \n",
    "\n",
    "### Evaluator Function (1 point)  \n",
    "Implements the evaluator function with Pytorch or Numpy only   \n",
    "- Evaluation metrics include confusion matrix, accuracy, recall score, precision and F1 score\n",
    "\n",
    "### Training and Evaluating Model1 (1 point)  \n",
    "Trains Model1 with customized hidden size, learning rate, number of iterations and batch size.\n",
    "Uses the predict function to predict the labels with the test dataset.\n",
    "Evaluates the prediction results.\n",
    "- Evaluation metrics include confusion matrix, accuracy, recall score, precision and F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e6a263",
   "metadata": {
    "id": "b7e6a263"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eKE_KANU7_sq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKE_KANU7_sq",
    "outputId": "946cd1ab-22bd-4efa-c734-faab97e6f40d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can go to Edit - Notebook settings to select GPU under the Hardware accelerator\n",
    "# check the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a5cbc46",
   "metadata": {
    "id": "9a5cbc46"
   },
   "outputs": [],
   "source": [
    "# build the dataset (train, validation and test)\n",
    "def load_MNIST(n_val=10000, n_sample=1000, sample=False):\n",
    "    n_val = n_val\n",
    "    n_sample = n_sample\n",
    "    train = MNIST(root = '.', train = True, download = True)\n",
    "    test = MNIST(root = '.', train = False, download = True)\n",
    "    \n",
    "    # data preprocessing\n",
    "    x_train, x_test = train.data/255, test.data/255\n",
    "    x_train = x_train.reshape(x_train.shape[0], -1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1)\n",
    "    y_train = torch.nn.functional.one_hot(train.targets)\n",
    "    y_test = torch.nn.functional.one_hot(test.targets)\n",
    "\n",
    "    data_dict = {}\n",
    "    if sample:\n",
    "        data_dict['x_train'] = x_train[:-n_val][:n_sample]\n",
    "        data_dict['y_train'] = y_train[:-n_val][:n_sample]\n",
    "        data_dict['x_val'] = x_train[-n_val:][:n_sample//10]\n",
    "        data_dict['y_val'] = y_train[-n_val:][:n_sample//10]\n",
    "        data_dict['x_test'] = x_test[:n_sample//10]\n",
    "        data_dict['y_test'] = y_test[:n_sample//10]\n",
    "    else:\n",
    "        data_dict['x_train'] = x_train[:-n_val]\n",
    "        data_dict['y_train'] = y_train[:-n_val]\n",
    "        data_dict['x_val'] = x_train[-n_val:]\n",
    "        data_dict['y_val'] = y_train[-n_val:]\n",
    "        data_dict['x_test'] = x_test\n",
    "        data_dict['y_test'] = y_test\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "Q5G-vmpD21Bj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "Q5G-vmpD21Bj",
    "outputId": "f053f294-9aad-4097-f75e-aaed70dd050e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: torch.Size([50000, 784])\n",
      "Train labels shape: torch.Size([50000, 10])\n",
      "Validation data shape: torch.Size([10000, 784])\n",
      "Validation labels shape: torch.Size([10000, 10])\n",
      "Test data shape: torch.Size([10000, 784])\n",
      "Test labels shape: torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "# you can start with a small sample dataset by setting sample=True\n",
    "data_dict = load_MNIST(sample=False)\n",
    "print('Train data shape:', data_dict['x_train'].shape)\n",
    "print('Train labels shape:', data_dict['y_train'].shape)\n",
    "print('Validation data shape:', data_dict['x_val'].shape)\n",
    "print('Validation labels shape:', data_dict['y_val'].shape)\n",
    "print('Test data shape:', data_dict['x_test'].shape)\n",
    "print('Test labels shape:', data_dict['y_test'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa9f016",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "1aa9f016",
    "outputId": "83a4195e-8fb9-4850-8a9a-7c3bbc2bb623"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot an example\n",
    "plt.imshow(data_dict['x_train'][0].reshape(28, 28))\n",
    "plt.title(data_dict['y_train'][0].argmax().item())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6291c647",
   "metadata": {
    "id": "6291c647"
   },
   "outputs": [],
   "source": [
    "def evaluator(y_test, y_pred):\n",
    "    ####################################################################################################\n",
    "    # enter code here to implement the evaluation metrics including confusion matrix, accuracy, precision and recall\n",
    "    # you can only use Numpy or Pytorch to implement the metrics\n",
    "    TN = FN = FP = TP = 0\n",
    "\n",
    "    for test, pred in zip(y_test, y_pred):\n",
    "        if test == 1: \n",
    "            if pred == 1: \n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        else:\n",
    "            if pred == 1:\n",
    "                FN += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "\n",
    "    accuracy = ( TP + TN ) / ( TP + FP + FN + TN )\n",
    "    precision = TP / ( TP + FP )\n",
    "    recall = TP / ( TP + FN )\n",
    "    f1 = 2 * precision * recall / ( precision + recall )\n",
    "\n",
    "    confusion_matrix = [[TN, FP], [FN, TP]]\n",
    "    evaluation_matrix = {'confusion_matrix': confusion_matrix,\n",
    "                         'accuracy': accuracy,\n",
    "                         'precision': precision,\n",
    "                         'recall': recall,\n",
    "                         'f1': f1}\n",
    "\n",
    "    return evaluation_matrix\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a0d3fc2d",
   "metadata": {
    "id": "a0d3fc2d"
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    def __init__(self, learning_rate, n_iters, batch_size, hidden_size, device, dtype=torch.float32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.history = {}\n",
    "        self.history['train_acc'], self.history['val_acc'], self.history['loss'] = [], [], []\n",
    "    \n",
    "    # 5. activation function\n",
    "    def sigmoid(self, x):\n",
    "        ####################################################################################################\n",
    "        # enter code here to implement the activation function\n",
    "        return 1 / ( 1 + torch.exp( -x ) )\n",
    "        \n",
    "        ####################################################################################################\n",
    "\n",
    "    def train(self, x, y, x_val, y_val, verbose=1):\n",
    "        n_train = x.shape[0]\n",
    "        n_val = x_val.shape[0]\n",
    "        input_size = x.shape[1]\n",
    "        num_classes = y.shape[1]\n",
    "        \n",
    "        # weight initialization\n",
    "        self.W1 = torch.randn(input_size, self.hidden_size, dtype=self.dtype, device=self.device) * 0.01\n",
    "        self.W2 = torch.randn(self.hidden_size, num_classes, dtype=self.dtype, device=self.device) * 0.01\n",
    "\n",
    "        # TODO: train the weights with the input data and labels\n",
    "        for i in range(self.n_iters):\n",
    "            loss = 0\n",
    "            data = getBatch(x, y, self.batch_size)\n",
    "            for x_batch, y_batch in data:\n",
    "                # 1. forward\n",
    "                ####################################################################################################\n",
    "                # enter code here to calculate the hidden layer output and output layer output\n",
    "                hidden = self.sigmoid( torch.matmul(x_batch,self.W1) )\n",
    "                output = self.sigmoid( torch.matmul(hidden,self.W2) )\n",
    "\n",
    "                ####################################################################################################\n",
    "\n",
    "                # 2. error and loss\n",
    "                ####################################################################################################\n",
    "                # enter code here to calculate the output error, MSE loss, delta output and delta hidden\n",
    "                output_error = y_batch - output\n",
    "                output_delta = torch.mul( output_error, np.multiply( output, (1 - output ) ) )\n",
    "\n",
    "                hidden_error = torch.matmul( output_delta, np.transpose( self.W2 ) )\n",
    "                hidden_delta = torch.multiply( hidden_error, np.multiply( hidden, (1 - hidden ) ) )\n",
    "\n",
    "                loss += torch.sum( torch.pow( output_error, 2 ) ) / self.batch_size\n",
    "\n",
    "                ####################################################################################################\n",
    "\n",
    "                # 3. backward\n",
    "                ####################################################################################################\n",
    "                # enter code here to calculate delta weights and update the weights\n",
    "                self.W1 += torch.matmul( torch.t( x_batch ), hidden_delta )\n",
    "                self.W2 += torch.matmul( torch.t( hidden ), output_delta )\n",
    "                \n",
    "                ####################################################################################################\n",
    "\n",
    "            # calculate the accuracy and save the training history\n",
    "            y_pred = self.predict(x)\n",
    "            train_acc = torch.sum(torch.argmax(y, dim=1) == y_pred) / n_train\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['loss'].append(loss)\n",
    "            \n",
    "            y_pred = self.predict(x_val)\n",
    "            val_acc = torch.sum(torch.argmax(y_val, dim=1) == y_pred) / n_val\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            if verbose:\n",
    "                print('epoch %d, loss %.4f, train acc %.3f, validation acc %.3f'\n",
    "                  % (i + 1, loss, train_acc, val_acc))\n",
    "    \n",
    "    # 4. predict function \n",
    "    def predict(self, x):\n",
    "        ####################################################################################################\n",
    "        # enter code here to implement the predict function\n",
    "        # TODO: use the trained weights to predict labels and return the predicted labels\n",
    "        # remember to use torch.argmax() to return the true labels\n",
    "        hidden = self.sigmoid( torch.matmul( x, self.W1 ) )\n",
    "        output = self.sigmoid( torch.matmul( hidden, self.W2 ) )\n",
    "        y_pred = torch.argmax( output, dim = 1 )\n",
    "        \n",
    "        ####################################################################################################\n",
    "        return y_pred\n",
    "\n",
    "def getBatch(x, y, batch_size):\n",
    "    n_epoch = x.shape[0] // batch_size\n",
    "    for i in range(n_epoch):\n",
    "        x_batch = x[i * batch_size : (i+1) * batch_size]\n",
    "        y_batch = y[i * batch_size : (i+1) * batch_size]\n",
    "        yield x_batch, y_batch\n",
    "    x_batch = x[(i+1) * batch_size:]\n",
    "    y_batch = y[(i+1) * batch_size:]    \n",
    "    yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "74e9819c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74e9819c",
    "outputId": "c6b1745f-4e19-4f83-afb7-c22f07062738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 1632.2755, train acc 0.931, validation acc 0.938\n",
      "epoch 2, loss 1022.3666, train acc 0.941, validation acc 0.943\n",
      "epoch 3, loss 903.2875, train acc 0.945, validation acc 0.948\n",
      "epoch 4, loss 848.7372, train acc 0.955, validation acc 0.951\n",
      "epoch 5, loss 780.8338, train acc 0.957, validation acc 0.953\n",
      "epoch 6, loss 761.2649, train acc 0.954, validation acc 0.954\n",
      "epoch 7, loss 711.7524, train acc 0.962, validation acc 0.959\n",
      "epoch 8, loss 689.6310, train acc 0.961, validation acc 0.956\n",
      "epoch 9, loss 654.4161, train acc 0.960, validation acc 0.957\n",
      "epoch 10, loss 618.1964, train acc 0.966, validation acc 0.961\n",
      "epoch 11, loss 605.7079, train acc 0.969, validation acc 0.961\n",
      "epoch 12, loss 600.7737, train acc 0.968, validation acc 0.958\n",
      "epoch 13, loss 576.5968, train acc 0.963, validation acc 0.955\n",
      "epoch 14, loss 570.7835, train acc 0.967, validation acc 0.957\n",
      "epoch 15, loss 575.6801, train acc 0.968, validation acc 0.959\n",
      "epoch 16, loss 544.3409, train acc 0.972, validation acc 0.962\n",
      "epoch 17, loss 526.5395, train acc 0.971, validation acc 0.961\n",
      "epoch 18, loss 522.4520, train acc 0.971, validation acc 0.960\n",
      "epoch 19, loss 494.9857, train acc 0.973, validation acc 0.961\n",
      "epoch 20, loss 504.4154, train acc 0.974, validation acc 0.962\n",
      "epoch 21, loss 486.9670, train acc 0.973, validation acc 0.959\n",
      "epoch 22, loss 481.1364, train acc 0.972, validation acc 0.961\n",
      "epoch 23, loss 464.6241, train acc 0.972, validation acc 0.958\n",
      "epoch 24, loss 464.3244, train acc 0.974, validation acc 0.962\n",
      "epoch 25, loss 464.0618, train acc 0.976, validation acc 0.962\n",
      "epoch 26, loss 454.3836, train acc 0.978, validation acc 0.964\n",
      "epoch 27, loss 426.9160, train acc 0.975, validation acc 0.962\n",
      "epoch 28, loss 414.2639, train acc 0.978, validation acc 0.964\n",
      "epoch 29, loss 439.4892, train acc 0.977, validation acc 0.964\n",
      "epoch 30, loss 418.3679, train acc 0.978, validation acc 0.962\n",
      "epoch 31, loss 403.0654, train acc 0.977, validation acc 0.962\n",
      "epoch 32, loss 397.4516, train acc 0.979, validation acc 0.965\n",
      "epoch 33, loss 406.9671, train acc 0.980, validation acc 0.964\n",
      "epoch 34, loss 383.2370, train acc 0.978, validation acc 0.963\n",
      "epoch 35, loss 383.3619, train acc 0.980, validation acc 0.962\n",
      "epoch 36, loss 370.9805, train acc 0.980, validation acc 0.963\n",
      "epoch 37, loss 372.0630, train acc 0.978, validation acc 0.963\n",
      "epoch 38, loss 359.4015, train acc 0.981, validation acc 0.965\n",
      "epoch 39, loss 348.4126, train acc 0.980, validation acc 0.963\n",
      "epoch 40, loss 344.2698, train acc 0.980, validation acc 0.964\n",
      "epoch 41, loss 361.1032, train acc 0.982, validation acc 0.963\n",
      "epoch 42, loss 353.9790, train acc 0.982, validation acc 0.963\n",
      "epoch 43, loss 353.6965, train acc 0.982, validation acc 0.962\n",
      "epoch 44, loss 336.4577, train acc 0.982, validation acc 0.963\n",
      "epoch 45, loss 342.5686, train acc 0.982, validation acc 0.963\n",
      "epoch 46, loss 336.6199, train acc 0.982, validation acc 0.963\n",
      "epoch 47, loss 339.1768, train acc 0.982, validation acc 0.963\n",
      "epoch 48, loss 329.4191, train acc 0.981, validation acc 0.963\n",
      "epoch 49, loss 320.6715, train acc 0.983, validation acc 0.964\n",
      "epoch 50, loss 328.4686, train acc 0.982, validation acc 0.961\n",
      "epoch 51, loss 314.3079, train acc 0.983, validation acc 0.963\n",
      "epoch 52, loss 308.2040, train acc 0.983, validation acc 0.965\n",
      "epoch 53, loss 315.8864, train acc 0.982, validation acc 0.965\n",
      "epoch 54, loss 332.7559, train acc 0.981, validation acc 0.963\n",
      "epoch 55, loss 335.0852, train acc 0.983, validation acc 0.962\n",
      "epoch 56, loss 307.1259, train acc 0.983, validation acc 0.964\n",
      "epoch 57, loss 291.5435, train acc 0.983, validation acc 0.963\n",
      "epoch 58, loss 298.5978, train acc 0.983, validation acc 0.965\n",
      "epoch 59, loss 275.4431, train acc 0.984, validation acc 0.965\n",
      "epoch 60, loss 287.4185, train acc 0.982, validation acc 0.963\n",
      "epoch 61, loss 280.8544, train acc 0.986, validation acc 0.965\n",
      "epoch 62, loss 282.0075, train acc 0.986, validation acc 0.965\n",
      "epoch 63, loss 272.0418, train acc 0.985, validation acc 0.964\n",
      "epoch 64, loss 265.0272, train acc 0.985, validation acc 0.962\n",
      "epoch 65, loss 261.8869, train acc 0.985, validation acc 0.963\n",
      "epoch 66, loss 255.1006, train acc 0.986, validation acc 0.964\n",
      "epoch 67, loss 256.3680, train acc 0.987, validation acc 0.964\n",
      "epoch 68, loss 249.7461, train acc 0.986, validation acc 0.965\n",
      "epoch 69, loss 274.6151, train acc 0.986, validation acc 0.964\n",
      "epoch 70, loss 245.2519, train acc 0.986, validation acc 0.963\n",
      "epoch 71, loss 253.9839, train acc 0.986, validation acc 0.964\n",
      "epoch 72, loss 253.8495, train acc 0.987, validation acc 0.964\n",
      "epoch 73, loss 245.1025, train acc 0.986, validation acc 0.963\n",
      "epoch 74, loss 266.4356, train acc 0.984, validation acc 0.963\n",
      "epoch 75, loss 263.6764, train acc 0.988, validation acc 0.965\n",
      "epoch 76, loss 236.4070, train acc 0.987, validation acc 0.966\n",
      "epoch 77, loss 244.2193, train acc 0.983, validation acc 0.961\n",
      "epoch 78, loss 250.8845, train acc 0.987, validation acc 0.964\n",
      "epoch 79, loss 244.8121, train acc 0.986, validation acc 0.964\n",
      "epoch 80, loss 232.2043, train acc 0.987, validation acc 0.963\n",
      "epoch 81, loss 224.4794, train acc 0.988, validation acc 0.965\n",
      "epoch 82, loss 215.5829, train acc 0.988, validation acc 0.963\n",
      "epoch 83, loss 235.7994, train acc 0.987, validation acc 0.964\n",
      "epoch 84, loss 223.1000, train acc 0.987, validation acc 0.964\n",
      "epoch 85, loss 212.6335, train acc 0.986, validation acc 0.965\n",
      "epoch 86, loss 237.2195, train acc 0.987, validation acc 0.964\n",
      "epoch 87, loss 228.0991, train acc 0.988, validation acc 0.966\n",
      "epoch 88, loss 205.9962, train acc 0.988, validation acc 0.963\n",
      "epoch 89, loss 205.3235, train acc 0.988, validation acc 0.963\n",
      "epoch 90, loss 212.6229, train acc 0.988, validation acc 0.965\n",
      "epoch 91, loss 208.1074, train acc 0.989, validation acc 0.966\n",
      "epoch 92, loss 199.7384, train acc 0.989, validation acc 0.966\n",
      "epoch 93, loss 200.3853, train acc 0.988, validation acc 0.965\n",
      "epoch 94, loss 190.6401, train acc 0.989, validation acc 0.966\n",
      "epoch 95, loss 198.3118, train acc 0.989, validation acc 0.966\n",
      "epoch 96, loss 203.8824, train acc 0.989, validation acc 0.964\n",
      "epoch 97, loss 189.7082, train acc 0.989, validation acc 0.964\n",
      "epoch 98, loss 175.3626, train acc 0.990, validation acc 0.965\n",
      "epoch 99, loss 183.2984, train acc 0.989, validation acc 0.965\n",
      "epoch 100, loss 177.6873, train acc 0.989, validation acc 0.966\n",
      "epoch 101, loss 187.6493, train acc 0.988, validation acc 0.964\n",
      "epoch 102, loss 194.0816, train acc 0.989, validation acc 0.963\n",
      "epoch 103, loss 179.1378, train acc 0.990, validation acc 0.966\n",
      "epoch 104, loss 194.1515, train acc 0.989, validation acc 0.965\n",
      "epoch 105, loss 181.8139, train acc 0.989, validation acc 0.966\n",
      "epoch 106, loss 175.8696, train acc 0.989, validation acc 0.964\n",
      "epoch 107, loss 179.5242, train acc 0.989, validation acc 0.965\n",
      "epoch 108, loss 204.5141, train acc 0.989, validation acc 0.964\n",
      "epoch 109, loss 183.1869, train acc 0.988, validation acc 0.966\n",
      "epoch 110, loss 196.6080, train acc 0.990, validation acc 0.965\n",
      "epoch 111, loss 180.6759, train acc 0.989, validation acc 0.964\n",
      "epoch 112, loss 190.0228, train acc 0.987, validation acc 0.963\n",
      "epoch 113, loss 173.8993, train acc 0.988, validation acc 0.966\n",
      "epoch 114, loss 184.8561, train acc 0.988, validation acc 0.963\n",
      "epoch 115, loss 178.7153, train acc 0.989, validation acc 0.964\n",
      "epoch 116, loss 180.1814, train acc 0.990, validation acc 0.966\n",
      "epoch 117, loss 161.7454, train acc 0.990, validation acc 0.965\n",
      "epoch 118, loss 169.5342, train acc 0.990, validation acc 0.964\n",
      "epoch 119, loss 172.4223, train acc 0.989, validation acc 0.964\n",
      "epoch 120, loss 173.4675, train acc 0.990, validation acc 0.965\n",
      "epoch 121, loss 155.3840, train acc 0.991, validation acc 0.965\n",
      "epoch 122, loss 156.4822, train acc 0.991, validation acc 0.966\n",
      "epoch 123, loss 158.0869, train acc 0.991, validation acc 0.967\n",
      "epoch 124, loss 169.2480, train acc 0.990, validation acc 0.966\n",
      "epoch 125, loss 155.4557, train acc 0.991, validation acc 0.966\n",
      "epoch 126, loss 147.2714, train acc 0.991, validation acc 0.966\n",
      "epoch 127, loss 154.4247, train acc 0.990, validation acc 0.966\n",
      "epoch 128, loss 171.6161, train acc 0.991, validation acc 0.966\n",
      "epoch 129, loss 156.4510, train acc 0.990, validation acc 0.967\n",
      "epoch 130, loss 172.7768, train acc 0.990, validation acc 0.966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 131, loss 159.9517, train acc 0.991, validation acc 0.965\n",
      "epoch 132, loss 164.9811, train acc 0.990, validation acc 0.966\n",
      "epoch 133, loss 177.6415, train acc 0.990, validation acc 0.965\n",
      "epoch 134, loss 193.5452, train acc 0.990, validation acc 0.966\n",
      "epoch 135, loss 181.7969, train acc 0.990, validation acc 0.965\n",
      "epoch 136, loss 166.3330, train acc 0.988, validation acc 0.964\n",
      "epoch 137, loss 196.0388, train acc 0.990, validation acc 0.965\n",
      "epoch 138, loss 173.8213, train acc 0.991, validation acc 0.966\n",
      "epoch 139, loss 174.3884, train acc 0.990, validation acc 0.966\n",
      "epoch 140, loss 173.4934, train acc 0.990, validation acc 0.966\n",
      "epoch 141, loss 155.8953, train acc 0.990, validation acc 0.964\n",
      "epoch 142, loss 163.6310, train acc 0.990, validation acc 0.965\n",
      "epoch 143, loss 163.2957, train acc 0.989, validation acc 0.965\n",
      "epoch 144, loss 158.5776, train acc 0.990, validation acc 0.965\n",
      "epoch 145, loss 161.9202, train acc 0.991, validation acc 0.965\n",
      "epoch 146, loss 151.4014, train acc 0.990, validation acc 0.965\n",
      "epoch 147, loss 163.0559, train acc 0.990, validation acc 0.964\n",
      "epoch 148, loss 153.5027, train acc 0.991, validation acc 0.964\n",
      "epoch 149, loss 147.5809, train acc 0.991, validation acc 0.964\n",
      "epoch 150, loss 150.6086, train acc 0.990, validation acc 0.965\n",
      "epoch 151, loss 166.7922, train acc 0.990, validation acc 0.965\n",
      "epoch 152, loss 161.8474, train acc 0.989, validation acc 0.965\n",
      "epoch 153, loss 165.8414, train acc 0.990, validation acc 0.966\n",
      "epoch 154, loss 176.4573, train acc 0.991, validation acc 0.965\n",
      "epoch 155, loss 145.3483, train acc 0.991, validation acc 0.965\n",
      "epoch 156, loss 144.8109, train acc 0.988, validation acc 0.966\n",
      "epoch 157, loss 143.8143, train acc 0.990, validation acc 0.965\n",
      "epoch 158, loss 162.3063, train acc 0.991, validation acc 0.966\n",
      "epoch 159, loss 158.3374, train acc 0.991, validation acc 0.966\n",
      "epoch 160, loss 147.3611, train acc 0.990, validation acc 0.966\n",
      "epoch 161, loss 142.6113, train acc 0.992, validation acc 0.966\n",
      "epoch 162, loss 138.7003, train acc 0.992, validation acc 0.967\n",
      "epoch 163, loss 141.2168, train acc 0.989, validation acc 0.965\n",
      "epoch 164, loss 143.2289, train acc 0.991, validation acc 0.965\n",
      "epoch 165, loss 139.9753, train acc 0.991, validation acc 0.965\n",
      "epoch 166, loss 142.8882, train acc 0.991, validation acc 0.967\n",
      "epoch 167, loss 154.7418, train acc 0.992, validation acc 0.966\n",
      "epoch 168, loss 147.5846, train acc 0.990, validation acc 0.966\n",
      "epoch 169, loss 139.7590, train acc 0.991, validation acc 0.965\n",
      "epoch 170, loss 133.0504, train acc 0.991, validation acc 0.965\n",
      "epoch 171, loss 145.8211, train acc 0.991, validation acc 0.966\n",
      "epoch 172, loss 135.1892, train acc 0.992, validation acc 0.966\n",
      "epoch 173, loss 139.8770, train acc 0.991, validation acc 0.965\n",
      "epoch 174, loss 135.8388, train acc 0.990, validation acc 0.964\n",
      "epoch 175, loss 142.7792, train acc 0.992, validation acc 0.966\n",
      "epoch 176, loss 140.3622, train acc 0.991, validation acc 0.965\n",
      "epoch 177, loss 133.2278, train acc 0.991, validation acc 0.966\n",
      "epoch 178, loss 138.0159, train acc 0.992, validation acc 0.966\n",
      "epoch 179, loss 129.1985, train acc 0.992, validation acc 0.965\n",
      "epoch 180, loss 134.9069, train acc 0.991, validation acc 0.965\n",
      "epoch 181, loss 135.5121, train acc 0.991, validation acc 0.966\n",
      "epoch 182, loss 134.8312, train acc 0.992, validation acc 0.966\n",
      "epoch 183, loss 133.0490, train acc 0.992, validation acc 0.966\n",
      "epoch 184, loss 119.9857, train acc 0.992, validation acc 0.965\n",
      "epoch 185, loss 117.1022, train acc 0.992, validation acc 0.965\n",
      "epoch 186, loss 117.2589, train acc 0.992, validation acc 0.967\n",
      "epoch 187, loss 122.5574, train acc 0.992, validation acc 0.966\n",
      "epoch 188, loss 117.1720, train acc 0.992, validation acc 0.966\n",
      "epoch 189, loss 114.1227, train acc 0.992, validation acc 0.967\n",
      "epoch 190, loss 120.1071, train acc 0.992, validation acc 0.966\n",
      "epoch 191, loss 116.4317, train acc 0.992, validation acc 0.966\n",
      "epoch 192, loss 127.5474, train acc 0.992, validation acc 0.967\n",
      "epoch 193, loss 123.4318, train acc 0.992, validation acc 0.967\n",
      "epoch 194, loss 126.8255, train acc 0.992, validation acc 0.967\n",
      "epoch 195, loss 118.0008, train acc 0.992, validation acc 0.967\n",
      "epoch 196, loss 115.5056, train acc 0.993, validation acc 0.967\n",
      "epoch 197, loss 121.1094, train acc 0.992, validation acc 0.967\n",
      "epoch 198, loss 124.2679, train acc 0.992, validation acc 0.966\n",
      "epoch 199, loss 119.4758, train acc 0.992, validation acc 0.965\n",
      "epoch 200, loss 133.9716, train acc 0.991, validation acc 0.963\n",
      "epoch 201, loss 140.0447, train acc 0.992, validation acc 0.964\n",
      "epoch 202, loss 140.9292, train acc 0.991, validation acc 0.964\n",
      "epoch 203, loss 128.4909, train acc 0.990, validation acc 0.965\n",
      "epoch 204, loss 144.8936, train acc 0.992, validation acc 0.965\n",
      "epoch 205, loss 128.2618, train acc 0.992, validation acc 0.966\n",
      "epoch 206, loss 132.7641, train acc 0.992, validation acc 0.968\n",
      "epoch 207, loss 127.0284, train acc 0.991, validation acc 0.965\n",
      "epoch 208, loss 137.7670, train acc 0.991, validation acc 0.966\n",
      "epoch 209, loss 136.4196, train acc 0.991, validation acc 0.965\n",
      "epoch 210, loss 139.4928, train acc 0.992, validation acc 0.967\n",
      "epoch 211, loss 138.4953, train acc 0.992, validation acc 0.967\n",
      "epoch 212, loss 133.9325, train acc 0.992, validation acc 0.967\n",
      "epoch 213, loss 123.8756, train acc 0.992, validation acc 0.967\n",
      "epoch 214, loss 123.9684, train acc 0.992, validation acc 0.966\n",
      "epoch 215, loss 134.9151, train acc 0.992, validation acc 0.967\n",
      "epoch 216, loss 126.1895, train acc 0.992, validation acc 0.966\n",
      "epoch 217, loss 137.3725, train acc 0.992, validation acc 0.966\n",
      "epoch 218, loss 156.1635, train acc 0.992, validation acc 0.965\n",
      "epoch 219, loss 143.5428, train acc 0.991, validation acc 0.965\n",
      "epoch 220, loss 137.9254, train acc 0.991, validation acc 0.965\n",
      "epoch 221, loss 134.0136, train acc 0.992, validation acc 0.966\n",
      "epoch 222, loss 140.9289, train acc 0.990, validation acc 0.964\n",
      "epoch 223, loss 144.0979, train acc 0.992, validation acc 0.966\n",
      "epoch 224, loss 137.8986, train acc 0.990, validation acc 0.965\n",
      "epoch 225, loss 138.4167, train acc 0.991, validation acc 0.968\n",
      "epoch 226, loss 126.0169, train acc 0.992, validation acc 0.966\n",
      "epoch 227, loss 121.5435, train acc 0.992, validation acc 0.967\n",
      "epoch 228, loss 119.8990, train acc 0.992, validation acc 0.965\n",
      "epoch 229, loss 124.4639, train acc 0.992, validation acc 0.966\n",
      "epoch 230, loss 127.2093, train acc 0.992, validation acc 0.966\n",
      "epoch 231, loss 119.7858, train acc 0.992, validation acc 0.966\n",
      "epoch 232, loss 125.3205, train acc 0.993, validation acc 0.966\n",
      "epoch 233, loss 114.1176, train acc 0.992, validation acc 0.966\n",
      "epoch 234, loss 120.5834, train acc 0.993, validation acc 0.966\n",
      "epoch 235, loss 113.5597, train acc 0.992, validation acc 0.965\n",
      "epoch 236, loss 113.2813, train acc 0.992, validation acc 0.966\n",
      "epoch 237, loss 117.7201, train acc 0.992, validation acc 0.967\n",
      "epoch 238, loss 149.5084, train acc 0.992, validation acc 0.965\n",
      "epoch 239, loss 137.1663, train acc 0.992, validation acc 0.965\n",
      "epoch 240, loss 130.7840, train acc 0.991, validation acc 0.965\n",
      "epoch 241, loss 134.3003, train acc 0.991, validation acc 0.964\n",
      "epoch 242, loss 131.2190, train acc 0.992, validation acc 0.967\n",
      "epoch 243, loss 130.7086, train acc 0.992, validation acc 0.966\n",
      "epoch 244, loss 134.1341, train acc 0.992, validation acc 0.965\n",
      "epoch 245, loss 130.0392, train acc 0.992, validation acc 0.966\n",
      "epoch 246, loss 135.9848, train acc 0.991, validation acc 0.965\n",
      "epoch 247, loss 122.1691, train acc 0.992, validation acc 0.966\n",
      "epoch 248, loss 125.9612, train acc 0.992, validation acc 0.965\n",
      "epoch 249, loss 156.0854, train acc 0.989, validation acc 0.961\n",
      "epoch 250, loss 176.4124, train acc 0.992, validation acc 0.965\n",
      "epoch 251, loss 151.3379, train acc 0.990, validation acc 0.964\n",
      "epoch 252, loss 149.0002, train acc 0.991, validation acc 0.965\n",
      "epoch 253, loss 155.9083, train acc 0.990, validation acc 0.965\n",
      "epoch 254, loss 161.2344, train acc 0.991, validation acc 0.965\n",
      "epoch 255, loss 144.9324, train acc 0.992, validation acc 0.966\n",
      "epoch 256, loss 141.7174, train acc 0.992, validation acc 0.967\n",
      "epoch 257, loss 151.7499, train acc 0.991, validation acc 0.965\n",
      "epoch 258, loss 139.3194, train acc 0.992, validation acc 0.966\n",
      "epoch 259, loss 124.0802, train acc 0.992, validation acc 0.966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 260, loss 117.1535, train acc 0.992, validation acc 0.966\n",
      "epoch 261, loss 115.3184, train acc 0.992, validation acc 0.967\n",
      "epoch 262, loss 124.7718, train acc 0.992, validation acc 0.966\n",
      "epoch 263, loss 121.4891, train acc 0.992, validation acc 0.966\n",
      "epoch 264, loss 139.1436, train acc 0.992, validation acc 0.966\n",
      "epoch 265, loss 122.9712, train acc 0.992, validation acc 0.966\n",
      "epoch 266, loss 117.7332, train acc 0.992, validation acc 0.965\n",
      "epoch 267, loss 132.9304, train acc 0.992, validation acc 0.967\n",
      "epoch 268, loss 138.8305, train acc 0.991, validation acc 0.964\n",
      "epoch 269, loss 144.3230, train acc 0.984, validation acc 0.961\n",
      "epoch 270, loss 180.7309, train acc 0.990, validation acc 0.965\n",
      "epoch 271, loss 146.6377, train acc 0.991, validation acc 0.966\n",
      "epoch 272, loss 135.9958, train acc 0.992, validation acc 0.966\n",
      "epoch 273, loss 147.4546, train acc 0.988, validation acc 0.961\n",
      "epoch 274, loss 188.6075, train acc 0.991, validation acc 0.964\n",
      "epoch 275, loss 145.5193, train acc 0.991, validation acc 0.966\n",
      "epoch 276, loss 138.3785, train acc 0.989, validation acc 0.964\n",
      "epoch 277, loss 130.6117, train acc 0.992, validation acc 0.967\n",
      "epoch 278, loss 124.9561, train acc 0.992, validation acc 0.966\n",
      "epoch 279, loss 138.0804, train acc 0.992, validation acc 0.965\n",
      "epoch 280, loss 135.0202, train acc 0.992, validation acc 0.965\n",
      "epoch 281, loss 127.7877, train acc 0.992, validation acc 0.966\n",
      "epoch 282, loss 128.8544, train acc 0.992, validation acc 0.965\n",
      "epoch 283, loss 120.3046, train acc 0.992, validation acc 0.966\n",
      "epoch 284, loss 112.2910, train acc 0.993, validation acc 0.966\n",
      "epoch 285, loss 109.8883, train acc 0.993, validation acc 0.965\n",
      "epoch 286, loss 107.5407, train acc 0.993, validation acc 0.965\n",
      "epoch 287, loss 107.1836, train acc 0.993, validation acc 0.965\n",
      "epoch 288, loss 105.3453, train acc 0.993, validation acc 0.965\n",
      "epoch 289, loss 104.3836, train acc 0.993, validation acc 0.965\n",
      "epoch 290, loss 103.8383, train acc 0.993, validation acc 0.965\n",
      "epoch 291, loss 103.2439, train acc 0.993, validation acc 0.965\n",
      "epoch 292, loss 103.1580, train acc 0.993, validation acc 0.966\n",
      "epoch 293, loss 102.8102, train acc 0.993, validation acc 0.966\n",
      "epoch 294, loss 102.6475, train acc 0.993, validation acc 0.965\n",
      "epoch 295, loss 102.7730, train acc 0.993, validation acc 0.965\n",
      "epoch 296, loss 102.3276, train acc 0.993, validation acc 0.965\n",
      "epoch 297, loss 102.1209, train acc 0.993, validation acc 0.965\n",
      "epoch 298, loss 101.8921, train acc 0.993, validation acc 0.965\n",
      "epoch 299, loss 101.8065, train acc 0.993, validation acc 0.965\n",
      "epoch 300, loss 101.5652, train acc 0.993, validation acc 0.965\n",
      "epoch 301, loss 101.4947, train acc 0.993, validation acc 0.965\n",
      "epoch 302, loss 101.3677, train acc 0.993, validation acc 0.965\n",
      "epoch 303, loss 101.6063, train acc 0.993, validation acc 0.965\n",
      "epoch 304, loss 101.8952, train acc 0.993, validation acc 0.965\n",
      "epoch 305, loss 101.2337, train acc 0.993, validation acc 0.965\n",
      "epoch 306, loss 101.0867, train acc 0.993, validation acc 0.965\n",
      "epoch 307, loss 101.0315, train acc 0.993, validation acc 0.965\n",
      "epoch 308, loss 101.2518, train acc 0.993, validation acc 0.965\n",
      "epoch 309, loss 100.9651, train acc 0.993, validation acc 0.965\n",
      "epoch 310, loss 100.9307, train acc 0.993, validation acc 0.965\n",
      "epoch 311, loss 100.8286, train acc 0.993, validation acc 0.965\n",
      "epoch 312, loss 100.7360, train acc 0.993, validation acc 0.965\n",
      "epoch 313, loss 100.6375, train acc 0.993, validation acc 0.965\n",
      "epoch 314, loss 100.5824, train acc 0.993, validation acc 0.965\n",
      "epoch 315, loss 100.5273, train acc 0.993, validation acc 0.965\n",
      "epoch 316, loss 100.4797, train acc 0.993, validation acc 0.965\n",
      "epoch 317, loss 100.4738, train acc 0.993, validation acc 0.965\n",
      "epoch 318, loss 100.4315, train acc 0.993, validation acc 0.965\n",
      "epoch 319, loss 100.3979, train acc 0.993, validation acc 0.965\n",
      "epoch 320, loss 100.3648, train acc 0.993, validation acc 0.965\n",
      "epoch 321, loss 100.3106, train acc 0.993, validation acc 0.965\n",
      "epoch 322, loss 100.3625, train acc 0.993, validation acc 0.965\n",
      "epoch 323, loss 100.3301, train acc 0.993, validation acc 0.965\n",
      "epoch 324, loss 101.3199, train acc 0.993, validation acc 0.965\n",
      "epoch 325, loss 101.3396, train acc 0.993, validation acc 0.965\n",
      "epoch 326, loss 101.9740, train acc 0.993, validation acc 0.965\n",
      "epoch 327, loss 107.0237, train acc 0.993, validation acc 0.965\n",
      "epoch 328, loss 102.3093, train acc 0.993, validation acc 0.965\n",
      "epoch 329, loss 104.2697, train acc 0.993, validation acc 0.966\n",
      "epoch 330, loss 105.2762, train acc 0.993, validation acc 0.967\n",
      "epoch 331, loss 101.0993, train acc 0.993, validation acc 0.967\n",
      "epoch 332, loss 101.2148, train acc 0.993, validation acc 0.967\n",
      "epoch 333, loss 101.7485, train acc 0.993, validation acc 0.966\n",
      "epoch 334, loss 103.0521, train acc 0.993, validation acc 0.966\n",
      "epoch 335, loss 101.2145, train acc 0.993, validation acc 0.965\n",
      "epoch 336, loss 100.3768, train acc 0.993, validation acc 0.965\n",
      "epoch 337, loss 103.5224, train acc 0.993, validation acc 0.965\n",
      "epoch 338, loss 116.2504, train acc 0.993, validation acc 0.966\n",
      "epoch 339, loss 117.7930, train acc 0.993, validation acc 0.966\n",
      "epoch 340, loss 116.9703, train acc 0.993, validation acc 0.966\n",
      "epoch 341, loss 111.5492, train acc 0.993, validation acc 0.966\n",
      "epoch 342, loss 105.0460, train acc 0.993, validation acc 0.966\n",
      "epoch 343, loss 101.9793, train acc 0.993, validation acc 0.966\n",
      "epoch 344, loss 100.8604, train acc 0.993, validation acc 0.966\n",
      "epoch 345, loss 100.6902, train acc 0.993, validation acc 0.966\n",
      "epoch 346, loss 100.1507, train acc 0.993, validation acc 0.966\n",
      "epoch 347, loss 99.9035, train acc 0.993, validation acc 0.966\n",
      "epoch 348, loss 99.7747, train acc 0.993, validation acc 0.966\n",
      "epoch 349, loss 99.6395, train acc 0.993, validation acc 0.966\n",
      "epoch 350, loss 99.6183, train acc 0.993, validation acc 0.966\n",
      "epoch 351, loss 99.5153, train acc 0.993, validation acc 0.965\n",
      "epoch 352, loss 99.4136, train acc 0.993, validation acc 0.965\n",
      "epoch 353, loss 99.3297, train acc 0.993, validation acc 0.965\n",
      "epoch 354, loss 100.4316, train acc 0.993, validation acc 0.965\n",
      "epoch 355, loss 100.1731, train acc 0.993, validation acc 0.965\n",
      "epoch 356, loss 99.4471, train acc 0.993, validation acc 0.965\n",
      "epoch 357, loss 99.1791, train acc 0.993, validation acc 0.965\n",
      "epoch 358, loss 99.1017, train acc 0.993, validation acc 0.965\n",
      "epoch 359, loss 99.2227, train acc 0.993, validation acc 0.965\n",
      "epoch 360, loss 99.0194, train acc 0.993, validation acc 0.966\n",
      "epoch 361, loss 98.9265, train acc 0.993, validation acc 0.966\n",
      "epoch 362, loss 99.4310, train acc 0.993, validation acc 0.965\n",
      "epoch 363, loss 100.4367, train acc 0.993, validation acc 0.966\n",
      "epoch 364, loss 107.8336, train acc 0.993, validation acc 0.965\n",
      "epoch 365, loss 118.5580, train acc 0.991, validation acc 0.964\n",
      "epoch 366, loss 134.6819, train acc 0.992, validation acc 0.965\n",
      "epoch 367, loss 147.3851, train acc 0.992, validation acc 0.965\n",
      "epoch 368, loss 144.0810, train acc 0.992, validation acc 0.964\n",
      "epoch 369, loss 140.8619, train acc 0.991, validation acc 0.966\n",
      "epoch 370, loss 134.8255, train acc 0.992, validation acc 0.964\n",
      "epoch 371, loss 121.5390, train acc 0.992, validation acc 0.966\n",
      "epoch 372, loss 120.0811, train acc 0.993, validation acc 0.966\n",
      "epoch 373, loss 117.2365, train acc 0.993, validation acc 0.966\n",
      "epoch 374, loss 119.2297, train acc 0.991, validation acc 0.966\n",
      "epoch 375, loss 111.5814, train acc 0.993, validation acc 0.966\n",
      "epoch 376, loss 111.5422, train acc 0.992, validation acc 0.966\n",
      "epoch 377, loss 129.3307, train acc 0.993, validation acc 0.966\n",
      "epoch 378, loss 121.8567, train acc 0.992, validation acc 0.966\n",
      "epoch 379, loss 129.5423, train acc 0.993, validation acc 0.966\n",
      "epoch 380, loss 123.2422, train acc 0.992, validation acc 0.966\n",
      "epoch 381, loss 134.0685, train acc 0.992, validation acc 0.965\n",
      "epoch 382, loss 156.7146, train acc 0.992, validation acc 0.965\n",
      "epoch 383, loss 129.9622, train acc 0.990, validation acc 0.966\n",
      "epoch 384, loss 155.4862, train acc 0.991, validation acc 0.965\n",
      "epoch 385, loss 180.0670, train acc 0.990, validation acc 0.964\n",
      "epoch 386, loss 160.5463, train acc 0.991, validation acc 0.964\n",
      "epoch 387, loss 160.4997, train acc 0.992, validation acc 0.964\n",
      "epoch 388, loss 146.9693, train acc 0.991, validation acc 0.965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 389, loss 149.2845, train acc 0.992, validation acc 0.964\n",
      "epoch 390, loss 160.3880, train acc 0.990, validation acc 0.963\n",
      "epoch 391, loss 151.6448, train acc 0.992, validation acc 0.965\n",
      "epoch 392, loss 141.8022, train acc 0.992, validation acc 0.965\n",
      "epoch 393, loss 146.1917, train acc 0.991, validation acc 0.965\n",
      "epoch 394, loss 145.1112, train acc 0.992, validation acc 0.966\n",
      "epoch 395, loss 126.5958, train acc 0.992, validation acc 0.966\n",
      "epoch 396, loss 124.2608, train acc 0.992, validation acc 0.965\n",
      "epoch 397, loss 144.1464, train acc 0.991, validation acc 0.965\n",
      "epoch 398, loss 152.1475, train acc 0.990, validation acc 0.964\n",
      "epoch 399, loss 122.7892, train acc 0.993, validation acc 0.966\n",
      "epoch 400, loss 136.0976, train acc 0.992, validation acc 0.965\n",
      "epoch 401, loss 135.9737, train acc 0.991, validation acc 0.964\n",
      "epoch 402, loss 137.7159, train acc 0.992, validation acc 0.964\n",
      "epoch 403, loss 117.6926, train acc 0.993, validation acc 0.963\n",
      "epoch 404, loss 120.9991, train acc 0.992, validation acc 0.965\n",
      "epoch 405, loss 131.9854, train acc 0.993, validation acc 0.965\n",
      "epoch 406, loss 128.9517, train acc 0.992, validation acc 0.965\n",
      "epoch 407, loss 116.5107, train acc 0.993, validation acc 0.966\n",
      "epoch 408, loss 110.8373, train acc 0.993, validation acc 0.965\n",
      "epoch 409, loss 108.7735, train acc 0.993, validation acc 0.965\n",
      "epoch 410, loss 110.3420, train acc 0.993, validation acc 0.965\n",
      "epoch 411, loss 108.2650, train acc 0.993, validation acc 0.965\n",
      "epoch 412, loss 107.6913, train acc 0.993, validation acc 0.965\n",
      "epoch 413, loss 103.7483, train acc 0.993, validation acc 0.966\n",
      "epoch 414, loss 109.4208, train acc 0.993, validation acc 0.965\n",
      "epoch 415, loss 120.1319, train acc 0.992, validation acc 0.965\n",
      "epoch 416, loss 127.0334, train acc 0.991, validation acc 0.962\n",
      "epoch 417, loss 116.2454, train acc 0.992, validation acc 0.964\n",
      "epoch 418, loss 116.8664, train acc 0.993, validation acc 0.966\n",
      "epoch 419, loss 117.5224, train acc 0.992, validation acc 0.965\n",
      "epoch 420, loss 130.0357, train acc 0.991, validation acc 0.963\n",
      "epoch 421, loss 115.6440, train acc 0.992, validation acc 0.965\n",
      "epoch 422, loss 115.0813, train acc 0.993, validation acc 0.965\n",
      "epoch 423, loss 109.7989, train acc 0.993, validation acc 0.965\n",
      "epoch 424, loss 105.8645, train acc 0.993, validation acc 0.965\n",
      "epoch 425, loss 109.6414, train acc 0.993, validation acc 0.966\n",
      "epoch 426, loss 103.2105, train acc 0.993, validation acc 0.966\n",
      "epoch 427, loss 104.0049, train acc 0.993, validation acc 0.966\n",
      "epoch 428, loss 100.6582, train acc 0.993, validation acc 0.966\n",
      "epoch 429, loss 99.1936, train acc 0.993, validation acc 0.966\n",
      "epoch 430, loss 98.9524, train acc 0.993, validation acc 0.966\n",
      "epoch 431, loss 98.7241, train acc 0.993, validation acc 0.966\n",
      "epoch 432, loss 99.2820, train acc 0.993, validation acc 0.965\n",
      "epoch 433, loss 99.6076, train acc 0.993, validation acc 0.966\n",
      "epoch 434, loss 99.1826, train acc 0.993, validation acc 0.966\n",
      "epoch 435, loss 98.4837, train acc 0.993, validation acc 0.966\n",
      "epoch 436, loss 99.8323, train acc 0.993, validation acc 0.965\n",
      "epoch 437, loss 98.4178, train acc 0.993, validation acc 0.965\n",
      "epoch 438, loss 98.1254, train acc 0.993, validation acc 0.965\n",
      "epoch 439, loss 97.9974, train acc 0.993, validation acc 0.965\n",
      "epoch 440, loss 98.0461, train acc 0.993, validation acc 0.965\n",
      "epoch 441, loss 102.3912, train acc 0.993, validation acc 0.966\n",
      "epoch 442, loss 102.1019, train acc 0.993, validation acc 0.965\n",
      "epoch 443, loss 115.9810, train acc 0.993, validation acc 0.965\n",
      "epoch 444, loss 121.6378, train acc 0.993, validation acc 0.965\n",
      "epoch 445, loss 130.9529, train acc 0.992, validation acc 0.964\n",
      "epoch 446, loss 114.0762, train acc 0.993, validation acc 0.965\n",
      "epoch 447, loss 117.5771, train acc 0.993, validation acc 0.965\n",
      "epoch 448, loss 111.5109, train acc 0.993, validation acc 0.965\n",
      "epoch 449, loss 105.7921, train acc 0.993, validation acc 0.965\n",
      "epoch 450, loss 106.5464, train acc 0.993, validation acc 0.966\n",
      "epoch 451, loss 110.7558, train acc 0.993, validation acc 0.965\n",
      "epoch 452, loss 102.7945, train acc 0.992, validation acc 0.965\n",
      "epoch 453, loss 103.6902, train acc 0.993, validation acc 0.965\n",
      "epoch 454, loss 113.4060, train acc 0.993, validation acc 0.965\n",
      "epoch 455, loss 115.2190, train acc 0.993, validation acc 0.965\n",
      "epoch 456, loss 119.9730, train acc 0.993, validation acc 0.965\n",
      "epoch 457, loss 106.0871, train acc 0.993, validation acc 0.964\n",
      "epoch 458, loss 108.2736, train acc 0.993, validation acc 0.965\n",
      "epoch 459, loss 105.0922, train acc 0.992, validation acc 0.964\n",
      "epoch 460, loss 123.1444, train acc 0.993, validation acc 0.965\n",
      "epoch 461, loss 117.0788, train acc 0.993, validation acc 0.964\n",
      "epoch 462, loss 112.7158, train acc 0.992, validation acc 0.964\n",
      "epoch 463, loss 118.2879, train acc 0.993, validation acc 0.965\n",
      "epoch 464, loss 117.7501, train acc 0.992, validation acc 0.963\n",
      "epoch 465, loss 111.9841, train acc 0.993, validation acc 0.963\n",
      "epoch 466, loss 107.2969, train acc 0.993, validation acc 0.964\n",
      "epoch 467, loss 106.7226, train acc 0.993, validation acc 0.964\n",
      "epoch 468, loss 114.3094, train acc 0.992, validation acc 0.964\n",
      "epoch 469, loss 110.9695, train acc 0.993, validation acc 0.965\n",
      "epoch 470, loss 102.6780, train acc 0.993, validation acc 0.965\n",
      "epoch 471, loss 99.6926, train acc 0.993, validation acc 0.965\n",
      "epoch 472, loss 98.5555, train acc 0.993, validation acc 0.965\n",
      "epoch 473, loss 99.0528, train acc 0.993, validation acc 0.965\n",
      "epoch 474, loss 97.6324, train acc 0.993, validation acc 0.965\n",
      "epoch 475, loss 97.1379, train acc 0.993, validation acc 0.965\n",
      "epoch 476, loss 97.0158, train acc 0.993, validation acc 0.965\n",
      "epoch 477, loss 97.1595, train acc 0.993, validation acc 0.965\n",
      "epoch 478, loss 96.6998, train acc 0.993, validation acc 0.965\n",
      "epoch 479, loss 96.5080, train acc 0.993, validation acc 0.965\n",
      "epoch 480, loss 96.3027, train acc 0.993, validation acc 0.965\n",
      "epoch 481, loss 97.4687, train acc 0.993, validation acc 0.965\n",
      "epoch 482, loss 98.0627, train acc 0.993, validation acc 0.965\n",
      "epoch 483, loss 96.9172, train acc 0.993, validation acc 0.965\n",
      "epoch 484, loss 96.9848, train acc 0.993, validation acc 0.965\n",
      "epoch 485, loss 96.4742, train acc 0.993, validation acc 0.965\n",
      "epoch 486, loss 96.2058, train acc 0.993, validation acc 0.965\n",
      "epoch 487, loss 96.1518, train acc 0.993, validation acc 0.965\n",
      "epoch 488, loss 96.1234, train acc 0.993, validation acc 0.965\n",
      "epoch 489, loss 96.1563, train acc 0.993, validation acc 0.966\n",
      "epoch 490, loss 103.0438, train acc 0.993, validation acc 0.965\n",
      "epoch 491, loss 109.2119, train acc 0.993, validation acc 0.964\n",
      "epoch 492, loss 104.4391, train acc 0.993, validation acc 0.965\n",
      "epoch 493, loss 106.5033, train acc 0.993, validation acc 0.964\n",
      "epoch 494, loss 109.3825, train acc 0.993, validation acc 0.965\n",
      "epoch 495, loss 100.8356, train acc 0.993, validation acc 0.965\n",
      "epoch 496, loss 105.6639, train acc 0.993, validation acc 0.964\n",
      "epoch 497, loss 110.0877, train acc 0.993, validation acc 0.965\n",
      "epoch 498, loss 115.4029, train acc 0.993, validation acc 0.964\n",
      "epoch 499, loss 118.8256, train acc 0.993, validation acc 0.967\n",
      "epoch 500, loss 127.6785, train acc 0.990, validation acc 0.963\n",
      "epoch 501, loss 159.6510, train acc 0.992, validation acc 0.964\n",
      "epoch 502, loss 129.3899, train acc 0.992, validation acc 0.964\n",
      "epoch 503, loss 137.0275, train acc 0.991, validation acc 0.965\n",
      "epoch 504, loss 144.4810, train acc 0.992, validation acc 0.964\n",
      "epoch 505, loss 132.8709, train acc 0.993, validation acc 0.965\n",
      "epoch 506, loss 116.7576, train acc 0.992, validation acc 0.964\n",
      "epoch 507, loss 138.1439, train acc 0.991, validation acc 0.963\n",
      "epoch 508, loss 129.3315, train acc 0.992, validation acc 0.965\n",
      "epoch 509, loss 128.8503, train acc 0.992, validation acc 0.964\n",
      "epoch 510, loss 143.3222, train acc 0.992, validation acc 0.965\n",
      "epoch 511, loss 132.2193, train acc 0.993, validation acc 0.964\n",
      "epoch 512, loss 129.6383, train acc 0.992, validation acc 0.965\n",
      "epoch 513, loss 129.1929, train acc 0.993, validation acc 0.965\n",
      "epoch 514, loss 126.9364, train acc 0.993, validation acc 0.965\n",
      "epoch 515, loss 124.2939, train acc 0.992, validation acc 0.965\n",
      "epoch 516, loss 129.8249, train acc 0.992, validation acc 0.965\n",
      "epoch 517, loss 138.8242, train acc 0.991, validation acc 0.963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 518, loss 123.9992, train acc 0.993, validation acc 0.966\n",
      "epoch 519, loss 114.5307, train acc 0.992, validation acc 0.964\n",
      "epoch 520, loss 120.6107, train acc 0.993, validation acc 0.965\n",
      "epoch 521, loss 141.8484, train acc 0.992, validation acc 0.964\n",
      "epoch 522, loss 132.5234, train acc 0.993, validation acc 0.964\n",
      "epoch 523, loss 116.4083, train acc 0.993, validation acc 0.965\n",
      "epoch 524, loss 112.9154, train acc 0.992, validation acc 0.964\n",
      "epoch 525, loss 136.7068, train acc 0.991, validation acc 0.965\n",
      "epoch 526, loss 145.3332, train acc 0.991, validation acc 0.965\n",
      "epoch 527, loss 142.5294, train acc 0.991, validation acc 0.963\n",
      "epoch 528, loss 157.4758, train acc 0.991, validation acc 0.964\n",
      "epoch 529, loss 162.3171, train acc 0.991, validation acc 0.966\n",
      "epoch 530, loss 151.6309, train acc 0.993, validation acc 0.966\n",
      "epoch 531, loss 132.8366, train acc 0.992, validation acc 0.967\n",
      "epoch 532, loss 151.8259, train acc 0.992, validation acc 0.965\n",
      "epoch 533, loss 156.9321, train acc 0.990, validation acc 0.963\n",
      "epoch 534, loss 167.4883, train acc 0.991, validation acc 0.964\n",
      "epoch 535, loss 161.5113, train acc 0.992, validation acc 0.965\n",
      "epoch 536, loss 140.7571, train acc 0.991, validation acc 0.964\n",
      "epoch 537, loss 163.7529, train acc 0.990, validation acc 0.965\n",
      "epoch 538, loss 155.9952, train acc 0.992, validation acc 0.963\n",
      "epoch 539, loss 135.8424, train acc 0.992, validation acc 0.965\n",
      "epoch 540, loss 168.9740, train acc 0.991, validation acc 0.964\n",
      "epoch 541, loss 151.8281, train acc 0.992, validation acc 0.966\n",
      "epoch 542, loss 136.0137, train acc 0.990, validation acc 0.963\n",
      "epoch 543, loss 148.6646, train acc 0.992, validation acc 0.966\n",
      "epoch 544, loss 124.5434, train acc 0.992, validation acc 0.965\n",
      "epoch 545, loss 131.2824, train acc 0.991, validation acc 0.963\n",
      "epoch 546, loss 163.0295, train acc 0.990, validation acc 0.964\n",
      "epoch 547, loss 147.6082, train acc 0.991, validation acc 0.963\n",
      "epoch 548, loss 148.5425, train acc 0.993, validation acc 0.964\n",
      "epoch 549, loss 127.0736, train acc 0.992, validation acc 0.965\n",
      "epoch 550, loss 133.0498, train acc 0.992, validation acc 0.966\n",
      "epoch 551, loss 126.8825, train acc 0.992, validation acc 0.964\n",
      "epoch 552, loss 144.5111, train acc 0.992, validation acc 0.964\n",
      "epoch 553, loss 109.7222, train acc 0.992, validation acc 0.964\n",
      "epoch 554, loss 115.5992, train acc 0.993, validation acc 0.964\n",
      "epoch 555, loss 109.3907, train acc 0.993, validation acc 0.964\n",
      "epoch 556, loss 120.3085, train acc 0.992, validation acc 0.963\n",
      "epoch 557, loss 143.0287, train acc 0.992, validation acc 0.963\n",
      "epoch 558, loss 150.7007, train acc 0.992, validation acc 0.963\n",
      "epoch 559, loss 154.8120, train acc 0.991, validation acc 0.963\n",
      "epoch 560, loss 155.6317, train acc 0.992, validation acc 0.965\n",
      "epoch 561, loss 142.7341, train acc 0.992, validation acc 0.963\n",
      "epoch 562, loss 132.4205, train acc 0.991, validation acc 0.963\n",
      "epoch 563, loss 139.1677, train acc 0.992, validation acc 0.964\n",
      "epoch 564, loss 131.2287, train acc 0.993, validation acc 0.965\n",
      "epoch 565, loss 128.3342, train acc 0.993, validation acc 0.964\n",
      "epoch 566, loss 142.1044, train acc 0.992, validation acc 0.965\n",
      "epoch 567, loss 151.5250, train acc 0.992, validation acc 0.964\n",
      "epoch 568, loss 131.5277, train acc 0.993, validation acc 0.965\n",
      "epoch 569, loss 123.6881, train acc 0.992, validation acc 0.964\n",
      "epoch 570, loss 149.9115, train acc 0.992, validation acc 0.965\n",
      "epoch 571, loss 147.0537, train acc 0.992, validation acc 0.964\n",
      "epoch 572, loss 134.9546, train acc 0.992, validation acc 0.965\n",
      "epoch 573, loss 146.1986, train acc 0.993, validation acc 0.965\n",
      "epoch 574, loss 127.5870, train acc 0.992, validation acc 0.966\n",
      "epoch 575, loss 114.3981, train acc 0.993, validation acc 0.965\n",
      "epoch 576, loss 116.5310, train acc 0.992, validation acc 0.965\n",
      "epoch 577, loss 110.5567, train acc 0.993, validation acc 0.965\n",
      "epoch 578, loss 111.7380, train acc 0.993, validation acc 0.965\n",
      "epoch 579, loss 108.8000, train acc 0.993, validation acc 0.965\n",
      "epoch 580, loss 107.8509, train acc 0.993, validation acc 0.966\n",
      "epoch 581, loss 113.3214, train acc 0.992, validation acc 0.965\n",
      "epoch 582, loss 111.8323, train acc 0.993, validation acc 0.966\n",
      "epoch 583, loss 105.1610, train acc 0.993, validation acc 0.966\n",
      "epoch 584, loss 103.1521, train acc 0.993, validation acc 0.965\n",
      "epoch 585, loss 103.0365, train acc 0.993, validation acc 0.966\n",
      "epoch 586, loss 101.6728, train acc 0.993, validation acc 0.965\n",
      "epoch 587, loss 100.0713, train acc 0.993, validation acc 0.965\n",
      "epoch 588, loss 100.6039, train acc 0.993, validation acc 0.965\n",
      "epoch 589, loss 97.1212, train acc 0.993, validation acc 0.966\n",
      "epoch 590, loss 97.4920, train acc 0.993, validation acc 0.966\n",
      "epoch 591, loss 95.6407, train acc 0.993, validation acc 0.965\n",
      "epoch 592, loss 95.4583, train acc 0.993, validation acc 0.965\n",
      "epoch 593, loss 95.3047, train acc 0.993, validation acc 0.965\n",
      "epoch 594, loss 95.1779, train acc 0.993, validation acc 0.965\n",
      "epoch 595, loss 95.0710, train acc 0.993, validation acc 0.965\n",
      "epoch 596, loss 94.9866, train acc 0.993, validation acc 0.965\n",
      "epoch 597, loss 96.9650, train acc 0.993, validation acc 0.965\n",
      "epoch 598, loss 95.6821, train acc 0.993, validation acc 0.965\n",
      "epoch 599, loss 94.8329, train acc 0.993, validation acc 0.965\n",
      "epoch 600, loss 94.7183, train acc 0.993, validation acc 0.965\n",
      "epoch 601, loss 94.6579, train acc 0.993, validation acc 0.965\n",
      "epoch 602, loss 94.6056, train acc 0.993, validation acc 0.965\n",
      "epoch 603, loss 94.5603, train acc 0.993, validation acc 0.965\n",
      "epoch 604, loss 94.5176, train acc 0.993, validation acc 0.965\n",
      "epoch 605, loss 94.4791, train acc 0.993, validation acc 0.965\n",
      "epoch 606, loss 94.4427, train acc 0.993, validation acc 0.965\n",
      "epoch 607, loss 94.4088, train acc 0.993, validation acc 0.965\n",
      "epoch 608, loss 94.3756, train acc 0.993, validation acc 0.965\n",
      "epoch 609, loss 94.3437, train acc 0.993, validation acc 0.965\n",
      "epoch 610, loss 94.3129, train acc 0.993, validation acc 0.965\n",
      "epoch 611, loss 94.2841, train acc 0.993, validation acc 0.965\n",
      "epoch 612, loss 94.2562, train acc 0.993, validation acc 0.965\n",
      "epoch 613, loss 94.2290, train acc 0.993, validation acc 0.965\n",
      "epoch 614, loss 94.2049, train acc 0.993, validation acc 0.965\n",
      "epoch 615, loss 98.5652, train acc 0.993, validation acc 0.966\n",
      "epoch 616, loss 97.2410, train acc 0.993, validation acc 0.966\n",
      "epoch 617, loss 98.2095, train acc 0.993, validation acc 0.966\n",
      "epoch 618, loss 110.2275, train acc 0.993, validation acc 0.966\n",
      "epoch 619, loss 116.9633, train acc 0.992, validation acc 0.965\n",
      "epoch 620, loss 118.5748, train acc 0.992, validation acc 0.965\n",
      "epoch 621, loss 109.7664, train acc 0.993, validation acc 0.965\n",
      "epoch 622, loss 106.3729, train acc 0.993, validation acc 0.966\n",
      "epoch 623, loss 122.1807, train acc 0.992, validation acc 0.965\n",
      "epoch 624, loss 113.3476, train acc 0.993, validation acc 0.967\n",
      "epoch 625, loss 107.2897, train acc 0.993, validation acc 0.966\n",
      "epoch 626, loss 101.3750, train acc 0.993, validation acc 0.966\n",
      "epoch 627, loss 102.2899, train acc 0.993, validation acc 0.966\n",
      "epoch 628, loss 98.2909, train acc 0.993, validation acc 0.965\n",
      "epoch 629, loss 98.2398, train acc 0.993, validation acc 0.965\n",
      "epoch 630, loss 105.1028, train acc 0.993, validation acc 0.965\n",
      "epoch 631, loss 109.6542, train acc 0.992, validation acc 0.966\n",
      "epoch 632, loss 121.9400, train acc 0.993, validation acc 0.966\n",
      "epoch 633, loss 106.5408, train acc 0.993, validation acc 0.965\n",
      "epoch 634, loss 109.1453, train acc 0.992, validation acc 0.965\n",
      "epoch 635, loss 109.0531, train acc 0.993, validation acc 0.966\n",
      "epoch 636, loss 114.2496, train acc 0.992, validation acc 0.964\n",
      "epoch 637, loss 129.9653, train acc 0.993, validation acc 0.965\n",
      "epoch 638, loss 124.1704, train acc 0.992, validation acc 0.964\n",
      "epoch 639, loss 145.9314, train acc 0.992, validation acc 0.965\n",
      "epoch 640, loss 128.2103, train acc 0.993, validation acc 0.965\n",
      "epoch 641, loss 116.1719, train acc 0.993, validation acc 0.965\n",
      "epoch 642, loss 114.0923, train acc 0.993, validation acc 0.966\n",
      "epoch 643, loss 101.1657, train acc 0.993, validation acc 0.965\n",
      "epoch 644, loss 98.5755, train acc 0.993, validation acc 0.966\n",
      "epoch 645, loss 95.5960, train acc 0.993, validation acc 0.966\n",
      "epoch 646, loss 94.6884, train acc 0.993, validation acc 0.966\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 647, loss 94.3139, train acc 0.993, validation acc 0.965\n",
      "epoch 648, loss 94.1521, train acc 0.993, validation acc 0.965\n",
      "epoch 649, loss 93.9302, train acc 0.993, validation acc 0.965\n",
      "epoch 650, loss 93.8337, train acc 0.993, validation acc 0.965\n",
      "epoch 651, loss 93.7445, train acc 0.993, validation acc 0.965\n",
      "epoch 652, loss 93.7544, train acc 0.993, validation acc 0.965\n",
      "epoch 653, loss 93.6091, train acc 0.993, validation acc 0.965\n",
      "epoch 654, loss 93.5591, train acc 0.993, validation acc 0.965\n",
      "epoch 655, loss 93.4541, train acc 0.993, validation acc 0.965\n",
      "epoch 656, loss 93.4438, train acc 0.993, validation acc 0.965\n",
      "epoch 657, loss 93.3434, train acc 0.993, validation acc 0.965\n",
      "epoch 658, loss 93.2962, train acc 0.993, validation acc 0.965\n",
      "epoch 659, loss 93.2652, train acc 0.993, validation acc 0.965\n",
      "epoch 660, loss 93.2538, train acc 0.993, validation acc 0.965\n",
      "epoch 661, loss 93.1968, train acc 0.993, validation acc 0.965\n",
      "epoch 662, loss 93.0321, train acc 0.993, validation acc 0.965\n",
      "epoch 663, loss 94.2890, train acc 0.993, validation acc 0.965\n",
      "epoch 664, loss 100.0621, train acc 0.993, validation acc 0.965\n",
      "epoch 665, loss 102.1744, train acc 0.993, validation acc 0.965\n",
      "epoch 666, loss 104.9043, train acc 0.993, validation acc 0.966\n",
      "epoch 667, loss 101.3442, train acc 0.993, validation acc 0.966\n",
      "epoch 668, loss 98.4711, train acc 0.993, validation acc 0.965\n",
      "epoch 669, loss 94.1813, train acc 0.993, validation acc 0.966\n",
      "epoch 670, loss 93.6908, train acc 0.993, validation acc 0.966\n",
      "epoch 671, loss 93.0650, train acc 0.993, validation acc 0.966\n",
      "epoch 672, loss 92.9251, train acc 0.993, validation acc 0.966\n",
      "epoch 673, loss 92.8101, train acc 0.993, validation acc 0.965\n",
      "epoch 674, loss 92.7473, train acc 0.993, validation acc 0.966\n",
      "epoch 675, loss 92.6853, train acc 0.993, validation acc 0.965\n",
      "epoch 676, loss 92.6524, train acc 0.993, validation acc 0.966\n",
      "epoch 677, loss 92.6374, train acc 0.993, validation acc 0.965\n",
      "epoch 678, loss 92.5602, train acc 0.993, validation acc 0.965\n",
      "epoch 679, loss 92.5287, train acc 0.993, validation acc 0.965\n",
      "epoch 680, loss 92.5179, train acc 0.993, validation acc 0.966\n",
      "epoch 681, loss 92.4772, train acc 0.993, validation acc 0.965\n",
      "epoch 682, loss 92.4344, train acc 0.993, validation acc 0.966\n",
      "epoch 683, loss 92.3590, train acc 0.993, validation acc 0.966\n",
      "epoch 684, loss 92.2227, train acc 0.993, validation acc 0.966\n",
      "epoch 685, loss 92.1917, train acc 0.993, validation acc 0.965\n",
      "epoch 686, loss 92.1609, train acc 0.993, validation acc 0.966\n",
      "epoch 687, loss 92.1356, train acc 0.993, validation acc 0.966\n",
      "epoch 688, loss 92.1233, train acc 0.993, validation acc 0.966\n",
      "epoch 689, loss 92.0789, train acc 0.993, validation acc 0.965\n",
      "epoch 690, loss 92.0665, train acc 0.993, validation acc 0.966\n",
      "epoch 691, loss 92.0063, train acc 0.993, validation acc 0.966\n",
      "epoch 692, loss 91.9759, train acc 0.993, validation acc 0.966\n",
      "epoch 693, loss 91.9335, train acc 0.993, validation acc 0.965\n",
      "epoch 694, loss 91.9039, train acc 0.993, validation acc 0.965\n",
      "epoch 695, loss 91.8713, train acc 0.993, validation acc 0.965\n",
      "epoch 696, loss 91.8443, train acc 0.993, validation acc 0.965\n",
      "epoch 697, loss 91.8462, train acc 0.993, validation acc 0.965\n",
      "epoch 698, loss 91.8430, train acc 0.993, validation acc 0.965\n",
      "epoch 699, loss 91.8063, train acc 0.993, validation acc 0.965\n",
      "epoch 700, loss 91.7863, train acc 0.993, validation acc 0.965\n",
      "epoch 701, loss 91.7589, train acc 0.993, validation acc 0.965\n",
      "epoch 702, loss 91.7378, train acc 0.993, validation acc 0.966\n",
      "epoch 703, loss 91.7038, train acc 0.993, validation acc 0.966\n",
      "epoch 704, loss 91.6846, train acc 0.993, validation acc 0.965\n",
      "epoch 705, loss 91.6691, train acc 0.993, validation acc 0.965\n",
      "epoch 706, loss 91.6552, train acc 0.993, validation acc 0.965\n",
      "epoch 707, loss 91.6312, train acc 0.993, validation acc 0.965\n",
      "epoch 708, loss 91.6163, train acc 0.993, validation acc 0.965\n",
      "epoch 709, loss 91.6015, train acc 0.993, validation acc 0.965\n",
      "epoch 710, loss 91.5882, train acc 0.993, validation acc 0.965\n",
      "epoch 711, loss 91.5646, train acc 0.993, validation acc 0.965\n",
      "epoch 712, loss 91.5490, train acc 0.993, validation acc 0.965\n",
      "epoch 713, loss 91.5350, train acc 0.993, validation acc 0.965\n",
      "epoch 714, loss 91.5196, train acc 0.993, validation acc 0.965\n",
      "epoch 715, loss 91.4929, train acc 0.993, validation acc 0.965\n",
      "epoch 716, loss 91.4751, train acc 0.993, validation acc 0.965\n",
      "epoch 717, loss 91.4582, train acc 0.993, validation acc 0.965\n",
      "epoch 718, loss 91.4428, train acc 0.993, validation acc 0.965\n",
      "epoch 719, loss 91.4202, train acc 0.993, validation acc 0.965\n",
      "epoch 720, loss 91.4030, train acc 0.993, validation acc 0.965\n",
      "epoch 721, loss 91.3878, train acc 0.993, validation acc 0.965\n",
      "epoch 722, loss 91.3740, train acc 0.993, validation acc 0.965\n",
      "epoch 723, loss 91.3450, train acc 0.993, validation acc 0.965\n",
      "epoch 724, loss 91.3279, train acc 0.993, validation acc 0.965\n",
      "epoch 725, loss 91.3193, train acc 0.993, validation acc 0.965\n",
      "epoch 726, loss 91.3094, train acc 0.993, validation acc 0.965\n",
      "epoch 727, loss 91.2807, train acc 0.993, validation acc 0.965\n",
      "epoch 728, loss 91.2679, train acc 0.993, validation acc 0.965\n",
      "epoch 729, loss 91.2626, train acc 0.993, validation acc 0.965\n",
      "epoch 730, loss 91.2553, train acc 0.993, validation acc 0.965\n",
      "epoch 731, loss 91.2377, train acc 0.993, validation acc 0.965\n",
      "epoch 732, loss 91.2301, train acc 0.993, validation acc 0.965\n",
      "epoch 733, loss 91.2179, train acc 0.993, validation acc 0.965\n",
      "epoch 734, loss 91.2099, train acc 0.993, validation acc 0.965\n",
      "epoch 735, loss 91.1894, train acc 0.993, validation acc 0.965\n",
      "epoch 736, loss 91.1729, train acc 0.993, validation acc 0.965\n",
      "epoch 737, loss 91.1625, train acc 0.993, validation acc 0.965\n",
      "epoch 738, loss 91.1537, train acc 0.993, validation acc 0.965\n",
      "epoch 739, loss 91.1392, train acc 0.993, validation acc 0.965\n",
      "epoch 740, loss 91.1281, train acc 0.993, validation acc 0.965\n",
      "epoch 741, loss 91.1209, train acc 0.993, validation acc 0.965\n",
      "epoch 742, loss 91.1105, train acc 0.993, validation acc 0.965\n",
      "epoch 743, loss 91.1000, train acc 0.993, validation acc 0.965\n",
      "epoch 744, loss 91.0903, train acc 0.993, validation acc 0.965\n",
      "epoch 745, loss 91.0791, train acc 0.993, validation acc 0.965\n",
      "epoch 746, loss 91.0685, train acc 0.993, validation acc 0.965\n",
      "epoch 747, loss 91.0616, train acc 0.993, validation acc 0.965\n",
      "epoch 748, loss 91.0512, train acc 0.993, validation acc 0.965\n",
      "epoch 749, loss 91.0420, train acc 0.993, validation acc 0.965\n",
      "epoch 750, loss 91.0275, train acc 0.993, validation acc 0.965\n",
      "epoch 751, loss 91.0210, train acc 0.993, validation acc 0.965\n",
      "epoch 752, loss 91.0092, train acc 0.993, validation acc 0.965\n",
      "epoch 753, loss 91.0041, train acc 0.993, validation acc 0.965\n",
      "epoch 754, loss 90.9887, train acc 0.993, validation acc 0.965\n",
      "epoch 755, loss 90.9836, train acc 0.993, validation acc 0.965\n",
      "epoch 756, loss 90.9728, train acc 0.993, validation acc 0.965\n",
      "epoch 757, loss 90.9690, train acc 0.993, validation acc 0.966\n",
      "epoch 758, loss 90.9568, train acc 0.993, validation acc 0.966\n",
      "epoch 759, loss 90.9508, train acc 0.993, validation acc 0.966\n",
      "epoch 760, loss 90.9402, train acc 0.993, validation acc 0.966\n",
      "epoch 761, loss 90.9355, train acc 0.993, validation acc 0.966\n",
      "epoch 762, loss 90.9263, train acc 0.993, validation acc 0.966\n",
      "epoch 763, loss 90.9181, train acc 0.993, validation acc 0.966\n",
      "epoch 764, loss 90.9089, train acc 0.993, validation acc 0.966\n",
      "epoch 765, loss 90.9048, train acc 0.993, validation acc 0.966\n",
      "epoch 766, loss 90.8975, train acc 0.993, validation acc 0.966\n",
      "epoch 767, loss 90.8909, train acc 0.993, validation acc 0.966\n",
      "epoch 768, loss 90.8789, train acc 0.993, validation acc 0.966\n",
      "epoch 769, loss 90.8681, train acc 0.993, validation acc 0.965\n",
      "epoch 770, loss 90.8518, train acc 0.993, validation acc 0.966\n",
      "epoch 771, loss 90.8383, train acc 0.993, validation acc 0.966\n",
      "epoch 772, loss 91.1545, train acc 0.993, validation acc 0.965\n",
      "epoch 773, loss 99.7922, train acc 0.993, validation acc 0.964\n",
      "epoch 774, loss 102.3073, train acc 0.993, validation acc 0.965\n",
      "epoch 775, loss 104.4165, train acc 0.993, validation acc 0.966\n",
      "epoch 776, loss 106.2460, train acc 0.992, validation acc 0.964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 777, loss 111.0927, train acc 0.993, validation acc 0.965\n",
      "epoch 778, loss 98.4904, train acc 0.993, validation acc 0.965\n",
      "epoch 779, loss 97.5856, train acc 0.993, validation acc 0.965\n",
      "epoch 780, loss 112.0925, train acc 0.993, validation acc 0.965\n",
      "epoch 781, loss 108.0154, train acc 0.993, validation acc 0.964\n",
      "epoch 782, loss 108.4413, train acc 0.993, validation acc 0.965\n",
      "epoch 783, loss 116.4204, train acc 0.993, validation acc 0.965\n",
      "epoch 784, loss 106.2443, train acc 0.993, validation acc 0.966\n",
      "epoch 785, loss 113.9086, train acc 0.993, validation acc 0.965\n",
      "epoch 786, loss 102.5663, train acc 0.993, validation acc 0.966\n",
      "epoch 787, loss 101.2990, train acc 0.993, validation acc 0.965\n",
      "epoch 788, loss 97.5069, train acc 0.993, validation acc 0.965\n",
      "epoch 789, loss 113.5907, train acc 0.993, validation acc 0.965\n",
      "epoch 790, loss 99.5340, train acc 0.993, validation acc 0.965\n",
      "epoch 791, loss 103.1027, train acc 0.993, validation acc 0.966\n",
      "epoch 792, loss 94.3596, train acc 0.993, validation acc 0.965\n",
      "epoch 793, loss 91.8674, train acc 0.993, validation acc 0.965\n",
      "epoch 794, loss 92.9249, train acc 0.993, validation acc 0.965\n",
      "epoch 795, loss 92.2762, train acc 0.993, validation acc 0.965\n",
      "epoch 796, loss 91.3602, train acc 0.993, validation acc 0.965\n",
      "epoch 797, loss 91.2201, train acc 0.993, validation acc 0.965\n",
      "epoch 798, loss 91.1167, train acc 0.993, validation acc 0.965\n",
      "epoch 799, loss 91.0344, train acc 0.993, validation acc 0.965\n",
      "epoch 800, loss 90.9450, train acc 0.993, validation acc 0.965\n",
      "epoch 801, loss 90.8863, train acc 0.993, validation acc 0.965\n",
      "epoch 802, loss 90.7917, train acc 0.993, validation acc 0.965\n",
      "epoch 803, loss 90.7509, train acc 0.993, validation acc 0.965\n",
      "epoch 804, loss 90.7271, train acc 0.993, validation acc 0.965\n",
      "epoch 805, loss 90.7039, train acc 0.993, validation acc 0.965\n",
      "epoch 806, loss 90.6473, train acc 0.993, validation acc 0.965\n",
      "epoch 807, loss 90.5952, train acc 0.993, validation acc 0.965\n",
      "epoch 808, loss 90.5954, train acc 0.993, validation acc 0.965\n",
      "epoch 809, loss 90.6264, train acc 0.993, validation acc 0.965\n",
      "epoch 810, loss 90.5169, train acc 0.993, validation acc 0.965\n",
      "epoch 811, loss 90.5075, train acc 0.993, validation acc 0.965\n",
      "epoch 812, loss 90.4845, train acc 0.993, validation acc 0.965\n",
      "epoch 813, loss 90.4716, train acc 0.993, validation acc 0.964\n",
      "epoch 814, loss 90.3934, train acc 0.993, validation acc 0.965\n",
      "epoch 815, loss 90.3814, train acc 0.993, validation acc 0.964\n",
      "epoch 816, loss 90.3432, train acc 0.993, validation acc 0.965\n",
      "epoch 817, loss 90.3264, train acc 0.993, validation acc 0.964\n",
      "epoch 818, loss 90.3195, train acc 0.993, validation acc 0.965\n",
      "epoch 819, loss 90.3024, train acc 0.993, validation acc 0.965\n",
      "epoch 820, loss 90.3433, train acc 0.993, validation acc 0.964\n",
      "epoch 821, loss 90.2101, train acc 0.993, validation acc 0.965\n",
      "epoch 822, loss 90.2346, train acc 0.993, validation acc 0.965\n",
      "epoch 823, loss 90.1727, train acc 0.993, validation acc 0.965\n",
      "epoch 824, loss 90.2409, train acc 0.993, validation acc 0.965\n",
      "epoch 825, loss 90.1936, train acc 0.993, validation acc 0.965\n",
      "epoch 826, loss 90.2004, train acc 0.993, validation acc 0.965\n",
      "epoch 827, loss 90.1369, train acc 0.993, validation acc 0.965\n",
      "epoch 828, loss 90.1740, train acc 0.993, validation acc 0.965\n",
      "epoch 829, loss 90.0827, train acc 0.993, validation acc 0.965\n",
      "epoch 830, loss 90.0743, train acc 0.993, validation acc 0.965\n",
      "epoch 831, loss 90.1295, train acc 0.993, validation acc 0.965\n",
      "epoch 832, loss 90.0416, train acc 0.993, validation acc 0.965\n",
      "epoch 833, loss 90.0760, train acc 0.993, validation acc 0.965\n",
      "epoch 834, loss 91.9296, train acc 0.993, validation acc 0.965\n",
      "epoch 835, loss 90.2411, train acc 0.993, validation acc 0.965\n",
      "epoch 836, loss 90.1462, train acc 0.993, validation acc 0.965\n",
      "epoch 837, loss 90.1554, train acc 0.993, validation acc 0.965\n",
      "epoch 838, loss 90.1549, train acc 0.993, validation acc 0.965\n",
      "epoch 839, loss 90.0739, train acc 0.993, validation acc 0.965\n",
      "epoch 840, loss 90.0260, train acc 0.993, validation acc 0.965\n",
      "epoch 841, loss 90.0773, train acc 0.993, validation acc 0.965\n",
      "epoch 842, loss 90.0385, train acc 0.993, validation acc 0.965\n",
      "epoch 843, loss 90.0055, train acc 0.993, validation acc 0.965\n",
      "epoch 844, loss 90.0607, train acc 0.993, validation acc 0.965\n",
      "epoch 845, loss 90.0115, train acc 0.993, validation acc 0.965\n",
      "epoch 846, loss 90.0046, train acc 0.993, validation acc 0.965\n",
      "epoch 847, loss 90.0154, train acc 0.993, validation acc 0.965\n",
      "epoch 848, loss 89.9643, train acc 0.993, validation acc 0.965\n",
      "epoch 849, loss 90.0603, train acc 0.993, validation acc 0.965\n",
      "epoch 850, loss 90.0019, train acc 0.993, validation acc 0.965\n",
      "epoch 851, loss 89.9378, train acc 0.993, validation acc 0.965\n",
      "epoch 852, loss 90.7812, train acc 0.993, validation acc 0.965\n",
      "epoch 853, loss 89.9632, train acc 0.993, validation acc 0.965\n",
      "epoch 854, loss 89.9293, train acc 0.993, validation acc 0.965\n",
      "epoch 855, loss 90.1110, train acc 0.993, validation acc 0.965\n",
      "epoch 856, loss 89.8211, train acc 0.993, validation acc 0.965\n",
      "epoch 857, loss 89.8484, train acc 0.993, validation acc 0.965\n",
      "epoch 858, loss 89.7719, train acc 0.993, validation acc 0.965\n",
      "epoch 859, loss 89.8100, train acc 0.993, validation acc 0.965\n",
      "epoch 860, loss 89.7532, train acc 0.993, validation acc 0.965\n",
      "epoch 861, loss 89.7915, train acc 0.993, validation acc 0.965\n",
      "epoch 862, loss 89.7300, train acc 0.993, validation acc 0.965\n",
      "epoch 863, loss 89.8911, train acc 0.993, validation acc 0.965\n",
      "epoch 864, loss 89.7930, train acc 0.993, validation acc 0.965\n",
      "epoch 865, loss 89.9526, train acc 0.993, validation acc 0.965\n",
      "epoch 866, loss 90.0459, train acc 0.993, validation acc 0.965\n",
      "epoch 867, loss 95.3792, train acc 0.993, validation acc 0.965\n",
      "epoch 868, loss 93.3501, train acc 0.993, validation acc 0.965\n",
      "epoch 869, loss 93.0818, train acc 0.993, validation acc 0.964\n",
      "epoch 870, loss 101.4746, train acc 0.992, validation acc 0.963\n",
      "epoch 871, loss 112.0438, train acc 0.993, validation acc 0.965\n",
      "epoch 872, loss 119.7064, train acc 0.993, validation acc 0.966\n",
      "epoch 873, loss 140.9336, train acc 0.993, validation acc 0.965\n",
      "epoch 874, loss 115.1846, train acc 0.993, validation acc 0.965\n",
      "epoch 875, loss 115.5077, train acc 0.993, validation acc 0.965\n",
      "epoch 876, loss 130.9098, train acc 0.992, validation acc 0.966\n",
      "epoch 877, loss 154.3865, train acc 0.991, validation acc 0.965\n",
      "epoch 878, loss 156.3118, train acc 0.992, validation acc 0.965\n",
      "epoch 879, loss 130.7569, train acc 0.992, validation acc 0.964\n",
      "epoch 880, loss 134.5849, train acc 0.993, validation acc 0.966\n",
      "epoch 881, loss 115.2657, train acc 0.993, validation acc 0.964\n",
      "epoch 882, loss 129.9621, train acc 0.991, validation acc 0.965\n",
      "epoch 883, loss 158.9984, train acc 0.992, validation acc 0.965\n",
      "epoch 884, loss 128.3814, train acc 0.993, validation acc 0.965\n",
      "epoch 885, loss 123.4930, train acc 0.992, validation acc 0.964\n",
      "epoch 886, loss 128.8385, train acc 0.992, validation acc 0.966\n",
      "epoch 887, loss 110.7517, train acc 0.993, validation acc 0.965\n",
      "epoch 888, loss 118.1003, train acc 0.992, validation acc 0.965\n",
      "epoch 889, loss 121.4870, train acc 0.993, validation acc 0.964\n",
      "epoch 890, loss 121.0655, train acc 0.992, validation acc 0.964\n",
      "epoch 891, loss 117.5553, train acc 0.993, validation acc 0.966\n",
      "epoch 892, loss 118.5542, train acc 0.993, validation acc 0.967\n",
      "epoch 893, loss 139.2505, train acc 0.991, validation acc 0.964\n",
      "epoch 894, loss 117.8323, train acc 0.993, validation acc 0.966\n",
      "epoch 895, loss 110.7724, train acc 0.993, validation acc 0.965\n",
      "epoch 896, loss 100.8018, train acc 0.993, validation acc 0.965\n",
      "epoch 897, loss 111.6827, train acc 0.993, validation acc 0.965\n",
      "epoch 898, loss 106.4372, train acc 0.993, validation acc 0.966\n",
      "epoch 899, loss 127.0668, train acc 0.993, validation acc 0.966\n",
      "epoch 900, loss 125.3607, train acc 0.993, validation acc 0.965\n",
      "epoch 901, loss 117.5757, train acc 0.993, validation acc 0.964\n",
      "epoch 902, loss 102.8067, train acc 0.993, validation acc 0.965\n",
      "epoch 903, loss 108.6426, train acc 0.993, validation acc 0.965\n",
      "epoch 904, loss 100.6181, train acc 0.993, validation acc 0.965\n",
      "epoch 905, loss 94.6166, train acc 0.993, validation acc 0.965\n",
      "epoch 906, loss 93.6584, train acc 0.993, validation acc 0.965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 907, loss 137.7109, train acc 0.991, validation acc 0.965\n",
      "epoch 908, loss 146.6967, train acc 0.993, validation acc 0.965\n",
      "epoch 909, loss 159.0026, train acc 0.991, validation acc 0.964\n",
      "epoch 910, loss 135.5379, train acc 0.991, validation acc 0.963\n",
      "epoch 911, loss 174.9820, train acc 0.991, validation acc 0.965\n",
      "epoch 912, loss 135.6326, train acc 0.992, validation acc 0.966\n",
      "epoch 913, loss 140.6139, train acc 0.993, validation acc 0.965\n",
      "epoch 914, loss 153.8327, train acc 0.992, validation acc 0.964\n",
      "epoch 915, loss 163.2350, train acc 0.992, validation acc 0.964\n",
      "epoch 916, loss 151.0223, train acc 0.992, validation acc 0.965\n",
      "epoch 917, loss 126.6869, train acc 0.993, validation acc 0.966\n",
      "epoch 918, loss 127.0080, train acc 0.992, validation acc 0.965\n",
      "epoch 919, loss 133.9351, train acc 0.993, validation acc 0.964\n",
      "epoch 920, loss 120.2339, train acc 0.992, validation acc 0.965\n",
      "epoch 921, loss 124.2144, train acc 0.992, validation acc 0.966\n",
      "epoch 922, loss 112.4319, train acc 0.993, validation acc 0.967\n",
      "epoch 923, loss 103.8410, train acc 0.993, validation acc 0.967\n",
      "epoch 924, loss 104.7027, train acc 0.993, validation acc 0.966\n",
      "epoch 925, loss 105.8518, train acc 0.993, validation acc 0.965\n",
      "epoch 926, loss 101.0587, train acc 0.993, validation acc 0.966\n",
      "epoch 927, loss 103.2687, train acc 0.993, validation acc 0.967\n",
      "epoch 928, loss 98.9827, train acc 0.993, validation acc 0.967\n",
      "epoch 929, loss 100.9025, train acc 0.992, validation acc 0.967\n",
      "epoch 930, loss 107.8559, train acc 0.993, validation acc 0.967\n",
      "epoch 931, loss 102.7845, train acc 0.993, validation acc 0.966\n",
      "epoch 932, loss 113.1247, train acc 0.993, validation acc 0.968\n",
      "epoch 933, loss 106.2330, train acc 0.993, validation acc 0.966\n",
      "epoch 934, loss 112.4803, train acc 0.993, validation acc 0.965\n",
      "epoch 935, loss 125.7392, train acc 0.992, validation acc 0.965\n",
      "epoch 936, loss 107.1763, train acc 0.993, validation acc 0.966\n",
      "epoch 937, loss 119.3107, train acc 0.993, validation acc 0.966\n",
      "epoch 938, loss 113.5874, train acc 0.993, validation acc 0.966\n",
      "epoch 939, loss 107.7750, train acc 0.993, validation acc 0.965\n",
      "epoch 940, loss 99.9661, train acc 0.993, validation acc 0.965\n",
      "epoch 941, loss 98.7098, train acc 0.993, validation acc 0.967\n",
      "epoch 942, loss 97.3236, train acc 0.993, validation acc 0.966\n",
      "epoch 943, loss 92.9325, train acc 0.993, validation acc 0.966\n",
      "epoch 944, loss 92.8166, train acc 0.993, validation acc 0.966\n",
      "epoch 945, loss 92.1977, train acc 0.993, validation acc 0.966\n",
      "epoch 946, loss 91.5452, train acc 0.993, validation acc 0.966\n",
      "epoch 947, loss 90.9980, train acc 0.993, validation acc 0.966\n",
      "epoch 948, loss 90.9440, train acc 0.993, validation acc 0.966\n",
      "epoch 949, loss 90.6477, train acc 0.993, validation acc 0.966\n",
      "epoch 950, loss 90.5161, train acc 0.993, validation acc 0.966\n",
      "epoch 951, loss 90.4367, train acc 0.993, validation acc 0.966\n",
      "epoch 952, loss 90.3699, train acc 0.993, validation acc 0.966\n",
      "epoch 953, loss 90.3128, train acc 0.993, validation acc 0.966\n",
      "epoch 954, loss 90.2619, train acc 0.993, validation acc 0.966\n",
      "epoch 955, loss 90.2179, train acc 0.993, validation acc 0.966\n",
      "epoch 956, loss 90.1873, train acc 0.993, validation acc 0.966\n",
      "epoch 957, loss 90.1404, train acc 0.993, validation acc 0.966\n",
      "epoch 958, loss 90.1055, train acc 0.993, validation acc 0.966\n",
      "epoch 959, loss 90.0663, train acc 0.993, validation acc 0.966\n",
      "epoch 960, loss 90.0081, train acc 0.993, validation acc 0.966\n",
      "epoch 961, loss 90.3130, train acc 0.993, validation acc 0.966\n",
      "epoch 962, loss 91.5769, train acc 0.993, validation acc 0.966\n",
      "epoch 963, loss 92.1302, train acc 0.993, validation acc 0.966\n",
      "epoch 964, loss 93.0698, train acc 0.993, validation acc 0.967\n",
      "epoch 965, loss 90.9460, train acc 0.993, validation acc 0.966\n",
      "epoch 966, loss 90.9249, train acc 0.993, validation acc 0.967\n",
      "epoch 967, loss 90.1414, train acc 0.993, validation acc 0.966\n",
      "epoch 968, loss 90.0143, train acc 0.993, validation acc 0.967\n",
      "epoch 969, loss 89.9011, train acc 0.993, validation acc 0.966\n",
      "epoch 970, loss 89.8412, train acc 0.993, validation acc 0.967\n",
      "epoch 971, loss 89.8159, train acc 0.993, validation acc 0.966\n",
      "epoch 972, loss 89.7861, train acc 0.993, validation acc 0.967\n",
      "epoch 973, loss 89.7731, train acc 0.993, validation acc 0.966\n",
      "epoch 974, loss 89.7391, train acc 0.993, validation acc 0.966\n",
      "epoch 975, loss 89.7163, train acc 0.993, validation acc 0.966\n",
      "epoch 976, loss 89.7152, train acc 0.993, validation acc 0.966\n",
      "epoch 977, loss 89.6989, train acc 0.993, validation acc 0.966\n",
      "epoch 978, loss 89.6807, train acc 0.993, validation acc 0.966\n",
      "epoch 979, loss 89.6706, train acc 0.993, validation acc 0.966\n",
      "epoch 980, loss 89.6395, train acc 0.993, validation acc 0.966\n",
      "epoch 981, loss 89.6249, train acc 0.993, validation acc 0.966\n",
      "epoch 982, loss 89.5995, train acc 0.993, validation acc 0.966\n",
      "epoch 983, loss 89.5807, train acc 0.993, validation acc 0.966\n",
      "epoch 984, loss 89.5609, train acc 0.993, validation acc 0.966\n",
      "epoch 985, loss 89.4915, train acc 0.993, validation acc 0.966\n",
      "epoch 986, loss 89.4396, train acc 0.993, validation acc 0.966\n",
      "epoch 987, loss 89.6427, train acc 0.993, validation acc 0.966\n",
      "epoch 988, loss 89.5319, train acc 0.993, validation acc 0.966\n",
      "epoch 989, loss 89.5647, train acc 0.993, validation acc 0.966\n",
      "epoch 990, loss 89.3676, train acc 0.993, validation acc 0.966\n",
      "epoch 991, loss 89.3424, train acc 0.993, validation acc 0.966\n",
      "epoch 992, loss 89.3214, train acc 0.993, validation acc 0.966\n",
      "epoch 993, loss 89.3030, train acc 0.993, validation acc 0.966\n",
      "epoch 994, loss 89.2878, train acc 0.993, validation acc 0.966\n",
      "epoch 995, loss 89.2723, train acc 0.993, validation acc 0.966\n",
      "epoch 996, loss 89.2616, train acc 0.993, validation acc 0.966\n",
      "epoch 997, loss 89.2460, train acc 0.993, validation acc 0.966\n",
      "epoch 998, loss 89.2370, train acc 0.993, validation acc 0.966\n",
      "epoch 999, loss 89.2236, train acc 0.993, validation acc 0.966\n",
      "epoch 1000, loss 89.2162, train acc 0.993, validation acc 0.966\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# enter code here to train Model1\n",
    "# TODO: set your desired hidden size, learning rate, number of iterations and batch size\n",
    "# remeber to load the dataset to the device (e.g. data_dict['x_train'].to(device))\n",
    "model = NN( learning_rate = 0.01,\n",
    "            n_iters = 1000,\n",
    "            batch_size = 5,\n",
    "            hidden_size = 50,\n",
    "            device = device )\n",
    "\n",
    "model.train( data_dict['x_train'].to(device), data_dict['y_train'].to(device), data_dict['x_val'].to(device), data_dict['y_val'].to(device))\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bebd0600",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "bebd0600",
    "outputId": "4cf59edb-b5bd-417c-913f-78b5a793189f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history['train_acc'], label='train_acc')\n",
    "plt.plot(model.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "UQciuqY96_6Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQciuqY96_6Z",
    "outputId": "0aff8095-e7a6-4558-f99c-36d2512488f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confusion_matrix': [[8840, 16], [25, 1119]], 'accuracy': 0.9959, 'precision': 0.9859030837004406, 'recall': 0.9781468531468531, 'f1': 0.9820096533567354}\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# enter code here to evaluate Model1 with test set\n",
    "# TODO: use the trained Model1 to predict the labels of test set and evaluate the results with the evaluator\n",
    "prediction = model.predict( data_dict['x_test'].to( device ) )\n",
    "evaluation_matrix = evaluator( data_dict['y_test'].to( device ).argmax( axis = 1), prediction )\n",
    "print( evaluation_matrix )\n",
    "                              \n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QG_rb7uJ4XDL",
   "metadata": {
    "id": "QG_rb7uJ4XDL"
   },
   "source": [
    "## Part 2\n",
    "Using another machine learning framework (**scikit-learn, Tensorflow and Pytorch**) to build MLP\n",
    "e.g. \n",
    "  1. https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "  2. https://www.tensorflow.org/api_docs/python/tf/keras/Sequential\n",
    "  3. https://pytorch.org/tutorials/beginner/examples_nn/polynomial_nn.html#sphx-glr-beginner-examples-nn-polynomial-nn-py\n",
    "  \n",
    "### Building Model2-1\n",
    "Implements Model2-1 with the same hidden nodes and optimization function as the model in Part 1.\n",
    "Train and validate model. Use the best model on validation dataset to test on the test dataset.\n",
    "\n",
    "### Training and Evaluating Model2-1\n",
    "Evaluates the prediction results  \n",
    "- Evaluation metrics include confusion matrix, accuracy, recall score, precision and F1 score.\n",
    "\n",
    "### Building Model2-2\n",
    "Adds one more hidden layer (2 hidden layers in total) to the model.\n",
    "Describes Model2-2 (number of hidden nodes)  \n",
    "Trains and validate model. Uses the best model on validation dataset to test on the test dataset.\n",
    "\n",
    "### Training and Evaluating Model2-2\n",
    "Evaluates the prediction results  \n",
    "- Evaluation metrics include confusion matrix, accuracy, recall score, precision and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "308de358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c845a9",
   "metadata": {},
   "source": [
    "### Model2-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "13519996",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# enter code here to implement Model2-1\n",
    "model1 = MLPClassifier( learning_rate_init = 0.01, \n",
    "                        max_iter = 1000,\n",
    "                        batch_size = 5,\n",
    "                        hidden_layer_sizes = (50,) )\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "208a8e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=5, hidden_layer_sizes=(50,), learning_rate_init=0.01,\n",
       "              max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=5, hidden_layer_sizes=(50,), learning_rate_init=0.01,\n",
       "              max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=5, hidden_layer_sizes=(50,), learning_rate_init=0.01,\n",
       "              max_iter=1000)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# enter code here to train Model2-1\n",
    "model1.fit( data_dict['x_train'].detach().cpu(), data_dict['y_train'].detach().cpu() )\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f61594f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confusion_matrix': array([[9.610e-02, 1.000e-04, 0.000e+00, 0.000e+00, 0.000e+00, 1.000e-04,\n",
      "        1.200e-03, 2.000e-04, 3.000e-04, 0.000e+00],\n",
      "       [2.300e-03, 1.102e-01, 2.000e-04, 1.000e-04, 0.000e+00, 0.000e+00,\n",
      "        2.000e-04, 0.000e+00, 5.000e-04, 0.000e+00],\n",
      "       [5.300e-03, 2.000e-04, 9.440e-02, 1.300e-03, 5.000e-04, 0.000e+00,\n",
      "        5.000e-04, 2.000e-04, 8.000e-04, 0.000e+00],\n",
      "       [4.900e-03, 0.000e+00, 6.000e-04, 9.400e-02, 0.000e+00, 1.000e-04,\n",
      "        0.000e+00, 2.000e-04, 9.000e-04, 3.000e-04],\n",
      "       [6.600e-03, 0.000e+00, 0.000e+00, 1.000e-04, 8.530e-02, 0.000e+00,\n",
      "        1.500e-03, 0.000e+00, 5.000e-04, 4.200e-03],\n",
      "       [1.280e-02, 0.000e+00, 0.000e+00, 4.200e-03, 0.000e+00, 6.600e-02,\n",
      "        1.900e-03, 3.000e-04, 3.800e-03, 2.000e-04],\n",
      "       [3.100e-03, 3.000e-04, 0.000e+00, 1.000e-04, 3.800e-03, 1.000e-04,\n",
      "        8.810e-02, 1.000e-04, 2.000e-04, 0.000e+00],\n",
      "       [9.800e-03, 2.000e-04, 1.800e-03, 2.000e-04, 5.000e-04, 0.000e+00,\n",
      "        0.000e+00, 8.690e-02, 1.000e-04, 3.300e-03],\n",
      "       [8.500e-03, 0.000e+00, 2.000e-04, 8.000e-04, 1.000e-03, 2.000e-04,\n",
      "        1.100e-03, 0.000e+00, 8.540e-02, 2.000e-04],\n",
      "       [6.700e-03, 3.000e-04, 0.000e+00, 3.000e-04, 1.200e-03, 0.000e+00,\n",
      "        1.000e-04, 3.000e-04, 8.000e-04, 9.120e-02]]), 'accuracy': 0.8901, 'precision': 0.9509585848956679, 'recall': 0.8978, 'f1': 0.9236150403785813}\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# enter code here to evaluate Model2-1\n",
    "prediction1 = model1.predict(data_dict['x_test'].detach().cpu())\n",
    "\n",
    "confusion_matrix1 = confusion_matrix(data_dict['y_test'].detach().cpu().argmax(axis=1), prediction1.argmax(axis=1), normalize='all')\n",
    "accuracy1 = accuracy_score( data_dict['y_test'].detach().cpu(), prediction1)\n",
    "precision1 = precision_score( data_dict['y_test'].detach().cpu(), prediction1, average = 'micro' )\n",
    "recall1 = recall_score( data_dict['y_test'].detach().cpu(), prediction1, average = 'micro' )\n",
    "f11 = f1_score( data_dict['y_test'].detach().cpu(), prediction1, average = 'micro' )\n",
    "\n",
    "evaluation_matrix1 = {'confusion_matrix': confusion_matrix1,\n",
    "                     'accuracy': accuracy1,\n",
    "                     'precision': precision1,\n",
    "                     'recall': recall1,\n",
    "                     'f1': f11}\n",
    "\n",
    "print(evaluation_matrix1)\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a9243f",
   "metadata": {},
   "source": [
    "### Model2-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0eaf38a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "# enter code here to implement Model2-2\n",
    "model2 = MLPClassifier( learning_rate_init = 0.01, \n",
    "                        max_iter = 1000,\n",
    "                        batch_size = 5,\n",
    "                        hidden_layer_sizes = (50,50) )\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "090300ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=5, hidden_layer_sizes=(50, 50),\n",
       "              learning_rate_init=0.01, max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(batch_size=5, hidden_layer_sizes=(50, 50),\n",
       "              learning_rate_init=0.01, max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(batch_size=5, hidden_layer_sizes=(50, 50),\n",
       "              learning_rate_init=0.01, max_iter=1000)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# enter code here to train Model2-2\n",
    "model2.fit( data_dict['x_train'].detach().cpu(), data_dict['y_train'].detach().cpu() )\n",
    "\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ca87daf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'confusion_matrix': array([[9.670e-02, 1.000e-04, 2.000e-04, 0.000e+00, 0.000e+00, 3.000e-04,\n",
      "        2.000e-04, 0.000e+00, 5.000e-04, 0.000e+00],\n",
      "       [9.000e-04, 1.113e-01, 1.000e-04, 7.000e-04, 0.000e+00, 1.000e-04,\n",
      "        0.000e+00, 0.000e+00, 4.000e-04, 0.000e+00],\n",
      "       [6.800e-03, 1.000e-04, 9.190e-02, 1.800e-03, 0.000e+00, 2.000e-04,\n",
      "        3.000e-04, 2.000e-04, 1.800e-03, 1.000e-04],\n",
      "       [3.400e-03, 0.000e+00, 8.000e-04, 9.370e-02, 0.000e+00, 1.100e-03,\n",
      "        0.000e+00, 2.000e-04, 1.300e-03, 5.000e-04],\n",
      "       [8.200e-03, 6.000e-04, 1.800e-03, 0.000e+00, 8.190e-02, 0.000e+00,\n",
      "        7.000e-04, 1.000e-04, 7.000e-04, 4.200e-03],\n",
      "       [5.600e-03, 1.000e-04, 1.000e-04, 2.700e-03, 0.000e+00, 7.900e-02,\n",
      "        4.000e-04, 2.000e-04, 7.000e-04, 4.000e-04],\n",
      "       [6.800e-03, 3.000e-04, 4.000e-04, 0.000e+00, 2.000e-04, 1.600e-03,\n",
      "        8.580e-02, 0.000e+00, 7.000e-04, 0.000e+00],\n",
      "       [7.100e-03, 1.000e-03, 1.900e-03, 0.000e+00, 3.000e-04, 1.000e-04,\n",
      "        0.000e+00, 8.870e-02, 3.000e-04, 3.400e-03],\n",
      "       [3.900e-03, 5.000e-04, 4.000e-04, 5.000e-04, 4.000e-04, 6.000e-04,\n",
      "        0.000e+00, 2.000e-04, 9.070e-02, 2.000e-04],\n",
      "       [4.400e-03, 4.000e-04, 2.000e-04, 5.000e-04, 8.000e-04, 3.000e-04,\n",
      "        0.000e+00, 2.000e-04, 8.000e-04, 9.330e-02]]), 'accuracy': 0.9087, 'precision': 0.9557447703143067, 'recall': 0.9092, 'f1': 0.9318915594731717}\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# enter code here to evaluate Model2-2\n",
    "prediction2 = model2.predict(data_dict['x_test'].detach().cpu())\n",
    "\n",
    "confusion_matrix2 = confusion_matrix(data_dict['y_test'].detach().cpu().argmax(axis=1), prediction2.argmax(axis=1), normalize='all')\n",
    "accuracy2 = accuracy_score( data_dict['y_test'].detach().cpu(), prediction2)\n",
    "precision2 = precision_score( data_dict['y_test'].detach().cpu(), prediction2, average = 'micro' )\n",
    "recall2 = recall_score( data_dict['y_test'].detach().cpu(), prediction2, average = 'micro' )\n",
    "f12 = f1_score( data_dict['y_test'].detach().cpu(), prediction2, average = 'micro' )\n",
    "\n",
    "evaluation_matrix2 = {'confusion_matrix': confusion_matrix2,\n",
    "                     'accuracy': accuracy2,\n",
    "                     'precision': precision2,\n",
    "                     'recall': recall2,\n",
    "                     'f1': f12}\n",
    "\n",
    "print(evaluation_matrix2)\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad554f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
